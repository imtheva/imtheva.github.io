<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> </head> <body> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Agricultural Robotics</abbr> <figure> <picture> <img src="/assets/img/publication_preview/CottonSim.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="CottonSim.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2025cottonsima" class="col-sm-8"> <div class="title">CottonSim: Development of an autonomous visual-guided robotic cotton-picking system in the Gazebo</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, Xin Zhang, Yanbo Huang, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jingdao Chen, Nuwan K Wijewardane, Vitor S Martins, Gary D Chesser, Christopher T Goodin' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2505.05317</em>. <em>More Information</em> can be <a href="https://github.com/imtheva/CottonSim" rel="external nofollow noopener" target="_blank">found here</a> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2505.05317" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Cotton is a major cash crop in the United States, with the country being a leading global producer and exporter. Nearly all U.S. cotton is grown in the Cotton Belt, spanning 17 states in the southern region. Harvesting remains a critical yet challenging stage, impacted by the use of costly, environmentally harmful defoliants and heavy, expensive cotton pickers. These factors contribute to yield loss, reduced fiber quality, and soil compaction, which collectively threaten long-term sustainability. To address these issues, this study proposes a lightweight, small-scale, vision-guided autonomous robotic cotton picker as an alternative. An autonomous system, built on Clearpath’s Husky platform and integrated with the CottonEye perception system, was developed and tested in the Gazebo simulation environment. A virtual cotton field was designed to facilitate autonomous navigation testing. The navigation system used Global Positioning System (GPS) and map-based guidance, assisted by an RGBdepth camera and a YOLOv8nseg instance segmentation model. The model achieved a mean Average Precision (mAP) of 85.2%, a recall of 88.9%, and a precision of 93.0%. The GPS-based approach reached a 100% completion rate (CR) within a threshold, while the map-based method achieved a 96.7% CR within a 0.25 m threshold. The developed Robot Operating System (ROS) packages enable robust simulation of autonomous cotton picking, offering a scalable baseline for future agricultural robotics. CottonSim code and datasets are publicly available on GitHub</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Agricultural Robotics</abbr> <figure> <picture> <img src="/assets/img/publication_preview/CottonSim.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="CottonSim.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2025cottonsimb" class="col-sm-8"> <div class="title">CottonSim: A vision-guided autonomous robotic system for cotton harvesting in Gazebo simulation</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, Xin Zhang, Yanbo Huang, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jingdao Chen, Nuwan K Wijewardane, Vitor S Martins, Gary D Chesser, Christopher T Goodin' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Computers and Electronics in Agriculture</em>. <em>More Information</em> can be <a href="https://github.com/imtheva/CottonSim" rel="external nofollow noopener" target="_blank">found here</a> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.compag.2025.110963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.compag.2025.110963" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.compag.2025.110963" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:hqOjcs7Dif8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Cotton is a major cash crop in the United States, with the country being a leading global producer and exporter. Nearly all U.S. cotton is grown in the Cotton Belt, spanning 17 states in the southern region. Harvesting remains a critical yet challenging stage, impacted by the use of costly, environmentally harmful defoliants and heavy, expensive cotton pickers. These factors contribute to yield loss, reduced fiber quality, and soil compaction, which collectively threaten long-term sustainability. To address these issues, this study proposes a lightweight, small-scale, vision-guided autonomous robotic cotton picker as an alternative. An autonomous system, built on Clearpath’s Husky platform and integrated with the CottonEye perception system, was developed and tested in the Gazebo simulation environment. A virtual cotton field was designed to facilitate autonomous navigation testing. The navigation system used Global Positioning System (GPS) and map-based guidance, assisted by an RGBdepth camera and a YOLOv8nseg instance segmentation model. The model achieved a mean Average Precision (mAP) of 85.2%, a recall of 88.9%, and a precision of 93.0%. The GPS-based approach reached a 100% completion rate (CR) within a threshold, while the map-based method achieved a 96.7% CR within a 0.25 m threshold. The developed Robot Operating System (ROS) packages enable robust simulation of autonomous cotton picking, offering a scalable baseline for future agricultural robotics. CottonSim code and datasets are publicly available on GitHub.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Computer Vision</abbr> <figure> <picture> <img src="/assets/img/publication_preview/blackberry.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="blackberry.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2025field" class="col-sm-8"> <div class="title">In-Field Multi-Ripeness Blackberry Detection for Soft Robotic Harvesting</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, Xin Zhang, Jonathan Harjono, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Yanbo Huang, Wenbo Liu, Amanda L McWhirt, Renee T Threlfall, Yue Chen' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Journal of the ASABE</em>. <em>More Information</em> can be <a href="https://github.com/Zhanglab-abe/Multi-Ripeness_Blackberry" rel="external nofollow noopener" target="_blank">found here</a> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.13031/ja.16300" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.13031/ja.16300" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.13031/ja.16300" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:0EnyYjriUFMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Blackberry harvesting is a crucial step for the fresh market production, requiring multiple passes of hand-picking because the berries do not ripen simultaneously, even on a plant, during the harvesting season. The blackberry harvesting process encounters several major hurdles in the U.S., including the shortage of agricultural labor and postharvest fruit quality since the blackberries are highly delicate and sensitive. Developing a computer vision-enabled robotic system for selectively picking blackberries can mitigate issues and secure the profitability of fresh blackberry growers. In-field blackberry detection and localization are extremely challenging attributable to several driving factors, such as the small size of the target berries, multiple levels of berry ripeness, and great variation of outdoor lighting conditions. This study aims to assess and compare the feasibility, accuracy, and efficiency of a series of state-of-the-art YOLO (You Only Look Once) models in detecting multi-ripeness blackberries in the farm conditions. A total of 1,086 images containing three different ripeness levels of blackberries were observed during the two-year harvesting season, including ripe berries (in black color), berries in the ripening stage (in pink color), and unripe berries (in green color). The computer vision pipeline developed in this study had the ability to detect and localize all berries at different ripeness levels, while detecting the ripe berries only was a particular focus. Overall, nine YOLO models (i.e., YOLOv5-x6, YOLOv6-l6, YOLOv7-base, YOLOv7-x, YOLOv7-e6e, YOLOv8-n, YOLOv8-x, YOLOv12-n, and YOLOv12-x) were trained and validated using randomly partitioned 760 (70%) and 108 images (10%), respectively. Among these models, YOLOv7-base outperformed the others in balancing mean Average Precision (mAP) and frames-per-second (FPS) on the test set, which comprised 218 images (20%). More specifically, YOLOv7-base achieved the mAP of 91.1%, F1-score of 84.9%, and inference speed of 12.6 ms per image with 1,024 x 1,024 pixels across all classes of ripeness. In addition, its mAP on the ripe berries was 92.4%, making YOLOv7-base a reliable tool for near real-time, in-field blackberry detection for soft robotic selective harvesting.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Agricultural Robotics</abbr> <figure> <picture> <img src="/assets/img/publication_preview/IFAC.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IFAC.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2025perception" class="col-sm-8"> <div class="title">Perception-enabled manipulator control for a robotic cotton picker with dual-side harvesting capability</div> <div class="author"> <em>Thevathayarajh Thayananthan</em> and Xin Zhang </div> <div class="periodical"> <em>IFAC-PapersOnLine</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.13031/ja.16300" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The United States is one of the leading global cotton producers, with most production concentrated in the southern states. Cotton harvesting predominantly relies on expensive and heavy mechanized harvesters, posing afordability challenges for small-scale farmers. Additionally, these machines require defoliant chemicals and contribute to soil compaction, raising environmental sustainability concerns. This study presents a robotic control system developed for a Universal Robot’s UR5e manipulator, designed to integrate with an autonomous cotton picker capable of dual-side harvesting. A dedicated control algorithm was implemented and tested using dual ZED 2i stereo cameras for cotton boll detection. Preliminary results confirmed the system’s accuracy and efficiency, demonstrating strong potential for robotic cotton harvesting applications.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Computer Vision</abbr> <figure> <picture> <img src="/assets/img/publication_preview/book_1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="book_1.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gunaratnam2024computer" class="col-sm-8"> <div class="title">Computer vision in livestock management and production</div> <div class="author"> Abhiram Gunaratnam, <em>Thevathayarajh Thayananthan</em>, Kartheeswaran Thangathurai, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Briyangari Abhiram' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Engineering Applications in Livestock Production</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/B978-0-323-98385-3.00002-5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/B978-0-323-98385-3.00002-5" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/B978-0-323-98385-3.00002-5" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:YsMSGLbcyi4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-9-4285F4?logo=googlescholar&amp;labelColor=beige" alt="9 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Intensification of livestock production systems is mainly driven by increasing population and subsequent increase in food demand. Major challenges of an intensified system are monitoring of a large number of animals, fast disease spread, frequent monitoring of health status, scarcity of labor, and optimizing the resources to maximize the profit. Different modern technologies are used to overcome these challenges; computer vision and image analysis techniques are the primary ones. The computer vision approach has gained prominence in contemporary livestock farming and has become an essential component of the farming system in many developed nations. This chapter provides insights into various applications of computer vision and image processing technologies (tracking of individual animals, health and disease monitoring, body size and weight measurement, and analysis of milk and meat quality), limitations, research gaps, and future prospects. This technology reduces labor requirements, eases management, is more precise in monitoring, and acts as a supporting tool for decision-making. However, the need for substantial initial capital, technical knowledge, regular monitoring, breed or strain-specific calibrations, and selection of correct features are identified as challenges of this technology. Lacks automation in computer vision technology, inadequate application of artificial intelligence in thermal imaging, identifying the optimum features for detection of lameness and lack of user-friendly applications are a few identified research gaps. Although computer vision has been implemented in livestock farming, it is still at the intermediate level and the identified challenges and research gaps need to be addressed through research and development for full-fledged automated livestock management.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Computer Vision</abbr> <figure> <picture> <img src="/assets/img/publication_preview/book_1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="book_1.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tarafdar2024engineering" class="col-sm-8"> <div class="title">Engineering Applications in Livestock Production</div> <div class="author"> Ayon Tarafdar, Ashok Pandey, Gyanendra Kumar Gaur, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Mukesh Singh, Hari Om Pandey' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/C2021-0-01048-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/C2021-0-01048-8" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/C2021-0-01048-8" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:WF5omc3nYNoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-6-4285F4?logo=googlescholar&amp;labelColor=beige" alt="6 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Engineering Applications in Livestock Production covers the recent advancements and technological developments in the field of livestock production engineering in great detail. The major advances covered in this book include the use of artificial intelligence, image processing, Internet of Things, novel animal product processing technologies, farm automation systems, sensor technology, bioengineering practices and even engineered housing systems among others.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Computer Vision</abbr> <figure> <picture> <img src="/assets/img/publication_preview/IEEE.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IEEE.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vimalambikaipakan2024traffic" class="col-sm-8"> <div class="title">Traffic sign recognition and auditory alert system for sri lankan drivers using deep-learning</div> <div class="author"> Geerthana Vimalambikaipakan, Chinthaka Amarasinghe, Tharangani Rajapaksha, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Thevathayarajh Thayananthan, Janotheepan Mariyathas' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 2024 International Research Conference on Smart Computing and Systems Engineering (SCSE)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/SCSE61872.2024.10550615" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/SCSE61872.2024.10550615" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/SCSE61872.2024.10550615" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Traffic signs are essential for road safety, guiding and informing drivers about rules and conditions. Ignoring signs or traffic rules can lead to accidents, causing property damage, and even deaths. To ensure safety, drivers must remain aware of traffic signs, obey their instructions, and drive responsibly. Notably, Sri Lanka has unique traffic signs compared to other countries. This study aims to develop a deep learning-based traffic sign alert system with good accuracy and efficiency in traffic sign recognition and enhance driver awareness by integrating an auditory alert system. This alert system was implemented under two main sections: object detection system and auditory alert system. The traffic signs dataset images were labeled and pre-processed using the Roboflow software tool. The You Only Look Once version 5 (YOLOv5) model was used to train the system, which allowed it to understand complicated patterns and attributes associated with 15 different sign classes. To improve driver awareness, an auditory alert system was developed using the PyDub library to generate real-time alerts in the form of voice messages. These alerts are triggered whenever a traffic sign is detected, giving an additional layer of information to the driver. On the test dataset, the integrated and fine-tuned YOLOv5 model achieved the F1 score of 90.09% and mean average precision (mAP) of 87.55% resulting in good evaluation metrics values for detection. This YOLOv5 model-based traffic sign auditory alert system was highly effective and efficient in increasing driver awareness and also helped to reduce road accidents.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Computer Vision</abbr> <figure> <picture> <img src="/assets/img/publication_preview/catfish.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="catfish.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2023automating" class="col-sm-8"> <div class="title">Automating catfish cutting process using deep learning-based semantic segmentation</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, Xin Zhang, Wenbo Liu, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Tianqi Yao, Yanbo Huang, Nuwan K Wijewardane, Yuzhen Lu' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In Sensing for Agriculture and Food Quality and Safety XV</em>. <em>More Information</em> can be <a href="https://github.com/Zhanglab-abe/Catfish-Segment" rel="external nofollow noopener" target="_blank">found here</a> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/12.2663370" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1117/12.2663370" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1117/12.2663370" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-2-4285F4?logo=googlescholar&amp;labelColor=beige" alt="2 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Mississippi and Alabama are the top two states producing and processing catfish in the United States, with the annual production of $382 million in 2022. The catfish industry supplies protein-rich catfish products to the U.S. market and contributes considerably to the development of the local economy. However, the traditional catfish processing heavily relies on human labors leading to a high demand of workforce in the processing facilities. De-heading, gutting, portioning, filleting, skinning, and trimming are the main steps of the catfish processing, which normally require blade-based cutting device (e.g., metal blades) to handle. The blade-based manual catfish processing might lead to product contamination, considerable fish meat waste, and low yield of catfish fillet depending on the workers’ skill levels. Furthermore, operating the cutting devices may expose the human labors to undesired work accidents. Therefore, automated catfish cutting process appears to be an alternative and promising solution with minimal involvement of human labors. To further enable, assist, and automate the catfish cutting technique in near real-time, this study presents a novel computer vision-based sensing system for segmenting the catfish into different target parts using deep learning and semantic segmentation. In this study, 396 raw and augmented catfish images were used to train, validate, and test five state-of-the-art deep learning semantic segmentation models, including BEiTV1, SegFormer-B0, SegFormer-B5, ViT-Adapter and PSPNet. Five classes were pre-defined for the segmentation, which could effectively guide the cutting system to locate the target, including the head, body, fins, tail of the catfish, and the image background. Overall, BEiTV1 demonstrated the poorest performance with 77.3% of mIoU (mean intersection-over-union) and 86.7% of MPA (mean pixel accuracy) among all tested models using the test data set, while SegFormer-B5 outperformed all others with 89.2% of mIoU and 94.6% of MPA on the catfish images. The inference speed for SegFormer-B5 was 0.278 sec per image at the resolution of 640x640. The proposed deep learning-based sensing system is expected to be a reliable tool for automating the catfish cutting process.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Computer Vision</abbr> <figure> <picture> <img src="/assets/img/publication_preview/blackberry.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="blackberry.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023multi" class="col-sm-8"> <div class="title">Multi-ripeness level blackberry detection using YOLOv7 for soft robotic harvesting</div> <div class="author"> Xin Zhang, <em>Thevathayarajh Thayananthan</em>, Muhammad Usman, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Wenbo Liu, Yue Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Autonomous Air and Ground Sensing Systems for Agricultural Optimization and Phenotyping VIII</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/12.2663367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1117/12.2663367" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1117/12.2663367" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Blackberry crop production is an essential sector of high-value specialty crops. Blackberries are delicate and easy to be damaged during harvest process. Besides, the blackberries in an orchard are not ripe at the same time so that multiple passes of harvesting are often needed. Therefore, the production is highly labor intensive and could be addressed using robotic solutions while maintaining the post-harvest berry quality for desired profitability. To further empower the developed tendon-driven soft robotic gripper specifically designed for berries, this study aims at investigating a state-of-the-art deep-learning YOLOv7 for accurately detecting the blackberries at multi-ripeness level in field conditions. In-field blackberry localization is a challenging task since blackberries are small objects and differ in color due to various levels of ripeness. Furthermore, the outdoor light condition varies depending on the time of day/location. Our study focused on detecting in-field blackberries at multi-ripeness levels using the state-of-the-art YOLOv7 model. In total, 642 RGB images were acquired targeting the plant canopies in several commercial orchards in Arkansas. The images were augmented to increase the diversity of data set using various methods. There are mainly three ripeness levels of blackberries that can present simultaneously in individual plants, including ripe (in black color), ripening (in red color), and unripe berries (in green color). The differentiation of ripeness levels can help the system to specifically harvest the ripe berries, and to keep track of the ripening/unripe berries in preparation for the next harvesting pass. The aggregation of total number of berries at all ripeness levels can also help estimate the crop-load for growers. The YOLOv7 model with seven configurations and six variants were trained and validated with 431 and 129 images, respectively. Overall, results of the test set (82 images) showed that YOLOv7-base was the best configuration with mean average precision (mAP) of 91.4% and F1-score of 0.86. YOLOv7-base also achieved 94% of mAP and 0.93 of True Positives (TPs) for ripe berries, 91% and 0.88 for ripening berries, and 88% and 0.86 for unripe berries under the Intersection-over-Union (IoU) of 0.5. The inference speed for YOLOv7-base was 21.5 ms on average per image with 1,024x1,024 resolution.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Robotics</abbr> <figure> <picture> <img src="/assets/img/publication_preview/IEEE.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IEEE.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yapa2023leveraging" class="col-sm-8"> <div class="title">Leveraging Virtual Reality for Robot Manipulator Education</div> <div class="author"> Aroshi Yapa, Kushan Ratnayake, Chanaka Prasad Premarathna, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Thevathayarajh Thayananthan' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2023 Moratuwa Engineering Research Conference (MERCon)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/MERCon60487.2023.10355455" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Equipping students with hands-on experience and knowledge of robot manipulators is crucial as robotics becomes more applicable in Industry 4.0. However, the practical implementation of physical manipulators in educational settings is challenging due to high capital costs, operational expertise requirements, safety considerations, and remote accessibility. This paper introduces an approach to address these obstacles by implementing a Virtual Reality (VR) platform for studying robot manipulators. The paper details the conceptual methodology, containing five domains: 3D Modelling, Application of Mechanics, VR Development, VR Integration, and Application Development. The methodology shows the creation of three VR applications focusing on distinct learning scenarios. The first application offers an overview of manipulators, while the second illustrates a specific operation (pick-and-place), providing insights into the combined function of links and joints. The third application allows users to interact with the manipulator, facilitating the execution of programming tasks. The paper concludes by outlining the benefits and the features of a VR manipulator over a physical robot manipulator. Then, it outlines the future enhancements, including developing a digital twin for the VR manipulator and a series of laboratory experiments. The study illustrates a crucial step towards broadening the accessibility of robotics education.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IoT</abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2021.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2021.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="danijal2021development" class="col-sm-8"> <div class="title">Development of Tracking over Speed System Using IoT Technology for Vehicles</div> <div class="author"> TJ Danijal and T Thevathayarajh </div> <div class="periodical"> <em></em> 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRCUWU2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Now, IoT technology is used for many applications. But they are certainly used for agriculture purpose in Sri Lanka. In the modern era, many accidents occur due to over speed. But accidents are controlled with the help of the police force and speed bumps in the past. IoT technology was used in this project to monitor the high speed. First, the velocity of the motorcycle was measured with the help of a Tachometer. When approaching the predefined maximum speed, the warning message was sent to the driver. When crossing the predefined maximum speed, all details including the reading of the Tachometer and location were sent to the mobile number of the motorcyclist and to the web application that was created for the police force at the same time using GPS/GPRS/GSM module. Finally, the output of the complete project shows 90% accuracy and the system showed the expected output with the available components. Keywords: IoT technology; over speed; Tachometer; Gps module; GPRS module; GSM module</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Mini-Project</abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2019.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2019.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thevathayarajh2019digital" class="col-sm-8"> <div class="title">A Digital Device to Measure Distance on Multiple Surfaces</div> <div class="author"> T Thevathayarajh, ARPCCJ Amarasinghe, and RMTCB Ekanayake </div> <div class="periodical"> <em></em> 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRCUWU2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Measurements are one of the daily procedures to separate the needed length in apparel industries. Measuring tape is the only available method for this process now a days. Even shoulder measurements in tailor shop is taken by measuring tape too. This might consumes time and causes error while taking measurements quickly. So there is a need for the independent measuring system to solve this deficiency. A pen like structure of diameter 3.5cm cylindrical wooden bar was cut and further diameter of 2cm was drilled in that where the components were set. A Keyes KY-040, rotary encoder was used to count the rotation was attached to the end of the cylinder. Smooth surface and rough surface shaped disc were made using 3D printers and those models were designed using SolidWorks software. These discs can be attached to the encoder shaft according to the surface where the device is used. Arduino Nano was used as microcontroller for performing algorithmic work. Liquid crystal display was used to display the output measurement. This measurement can be displayed in Centimeter, Foot and Inch scales at the press of a button. Error corrections were done according to the measurements obtained from the Measuring device. 96% percentage accuracy was obtained on the random measurements</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Computer Vision</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Indian_conf.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Indian_conf.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2019digital" class="col-sm-8"> <div class="title">A digital device to monitor the colour changes during fermentation stage of black tea processing</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, D.D.C. Wanniarachchi, and K.W.S.N. Kumari </div> <div class="periodical"> <em>In International Conference on Recent Advances in Computer Science and Engineering (IC-RACE) 2019</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Indian_conf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The tea industry is one of the most economically important agricultural sectors worldwide, where tea quality is primarily influenced by the fermentation process. Accurate assessment of tea fermentation is essential for maintaining consistent quality and is commonly evaluated using Theaflavin (TF) and Thearubigin (TR) ratios, with high-quality tea typically exhibiting TF/TR ratios between 1 and 1/10. These quality indicators are derived from polyphenols formed during fermentation and are affected by environmental and chemical conditions within the fermentation bed. In practice, tea makers rely on visual inspection of leaf colour changes to estimate fermentation completion, a method that depends heavily on experience and may lead to inconsistencies. This study proposes a novel sensor-based approach to establish a quantitative relationship between leaf colour variation and fermentation conditions. A device equipped with sensors measuring temperature, humidity, moisture, and colour was developed and deployed on the fermentation bed. Data were collected at five-minute intervals from multiple fermentation batches, with colour variations monitored across the fermentation duration. Reference samples were simultaneously analysed using spectrophotometric methods to obtain TF and TR values, along with UV spectral data. The collected data were statistically analysed to predict the relationship between fermentation parameters and tea quality indicators. Results show that average temperature, temperature variation, and room conditions significantly influence fermentation time at a 0.05 significance level, with the proposed method achieving an R² value of 96.6%. The findings demonstrate that sensor-driven colour analysis provides a reliable and objective alternative for monitoring tea fermentation and predicting optimal fermentation completion.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Computer Vision</abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2018.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2018development" class="col-sm-8"> <div class="title">Development of a monitoring device for fermentation stage of black tea manufacturing</div> <div class="author"> <em>Thevathayarajh Thayananthan</em> and D.D.C. Wanniarachchi </div> <div class="periodical"> <em>In 2nd International Research Symposium</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRCUWU2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Tea is one of the popular industries around the world for its social influence. Taste,tea colour and odor are the ways to measure the quality on the final product, but Theaflavins/Thearubigins ratio is accepted as 1/10 for a high quality. In general factory officers estimate the optimum fermentation time visually. However, optimum fermentation time might depend on humidity, temperature, moisture content of leaves, which are processing on a particular day. Thus, there is a need for a system independent from human decision. An electronic device was developed, in order to determine the optimum fermentation time for the black tea. A set of sensors such as humidity, moisture and temperature were attached to the device to store the physical environment data of the fermentation bed. Colour changes during the fermentation were monitored using an iPhone 6s camera, which have 12 mega pixels. First set of tea particles from a batch of fermentation was selected for the research. This device was allowed to collect data until the factory officer asked to stop the fermentation according to his own decision. A sample of each monitored batch was collected after firing to measure percentage of Theaflavins. Tea infusion was monitored using Ultraviolet spectrophotometer. Finally, data were analyzed statistically. Theaflavins content decreases with the fermentation time which is the trend expected. The average temperature, temperature difference and average room temperature are statistically significant with fermentation time at 0.05 level of significance and percentage of variance is 96.6. Moisture content is constant because this is expected as we focused on dry season leaves. Finally, study confirms the importance of measuring physical parameters when monitoring fermentation stage to obtain quality tea. Some advanced test has to be done on the fermentation tea colour in future. </p> </div> </div> </div> </li></ol> </div> </body> </html>