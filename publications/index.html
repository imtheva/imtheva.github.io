<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Thevathayarajh Thayananthan </title> <meta name="author" content="Thevathayarajh Thayananthan"> <meta name="description" content="Publications including journal articles, conference papers, and book chapters, organized by category in reverse chronological order."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo-1.png?5f79aa8a9c5775ce3c181141561fab99"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://imtheva.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script async src="//cdn.plu.mx/widget-popup.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Thevathayarajh</span> Thayananthan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/news/">News</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/teaching/">Teaching</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/repositories/">Repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/gallery/">Gallery</a> <div class="dropdown-divider"></div> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">Publications including journal articles, conference papers, and book chapters, organized by category in reverse chronological order.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #FFF7CC; --venue-color-dark: #736100;"> <div>Agricultural Robotics</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/CottonSim.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="CottonSim.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2025cottonsima" class="col-sm-8"> <div class="title">CottonSim: Development of an autonomous visual-guided robotic cotton-picking system in the Gazebo</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, Xin Zhang, Yanbo Huang, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jingdao Chen, Nuwan K Wijewardane, Vitor S Martins, Gary D Chesser, Christopher T Goodin' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2505.05317</em>. <em>More Information</em> can be <a href="https://github.com/imtheva/CottonSim" rel="external nofollow noopener" target="_blank">found here</a> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2505.05317" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Cotton is a major cash crop in the United States, with the country being a leading global producer and exporter. Nearly all U.S. cotton is grown in the Cotton Belt, spanning 17 states in the southern region. Harvesting remains a critical yet challenging stage, impacted by the use of costly, environmentally harmful defoliants and heavy, expensive cotton pickers. These factors contribute to yield loss, reduced fiber quality, and soil compaction, which collectively threaten long-term sustainability. To address these issues, this study proposes a lightweight, small-scale, vision-guided autonomous robotic cotton picker as an alternative. An autonomous system, built on Clearpath’s Husky platform and integrated with the CottonEye perception system, was developed and tested in the Gazebo simulation environment. A virtual cotton field was designed to facilitate autonomous navigation testing. The navigation system used Global Positioning System (GPS) and map-based guidance, assisted by an RGBdepth camera and a YOLOv8nseg instance segmentation model. The model achieved a mean Average Precision (mAP) of 85.2%, a recall of 88.9%, and a precision of 93.0%. The GPS-based approach reached a 100% completion rate (CR) within a threshold, while the map-based method achieved a 96.7% CR within a 0.25 m threshold. The developed Robot Operating System (ROS) packages enable robust simulation of autonomous cotton picking, offering a scalable baseline for future agricultural robotics. CottonSim code and datasets are publicly available on GitHub</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #FFF7CC; --venue-color-dark: #736100;"> <div>Agricultural Robotics</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/CottonSim.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="CottonSim.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2025cottonsimb" class="col-sm-8"> <div class="title">CottonSim: A vision-guided autonomous robotic system for cotton harvesting in Gazebo simulation</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, Xin Zhang, Yanbo Huang, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jingdao Chen, Nuwan K Wijewardane, Vitor S Martins, Gary D Chesser, Christopher T Goodin' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Computers and Electronics in Agriculture</em>. <em>More Information</em> can be <a href="https://github.com/imtheva/CottonSim" rel="external nofollow noopener" target="_blank">found here</a> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.compag.2025.110963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.compag.2025.110963" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.compag.2025.110963" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:hqOjcs7Dif8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Cotton is a major cash crop in the United States, with the country being a leading global producer and exporter. Nearly all U.S. cotton is grown in the Cotton Belt, spanning 17 states in the southern region. Harvesting remains a critical yet challenging stage, impacted by the use of costly, environmentally harmful defoliants and heavy, expensive cotton pickers. These factors contribute to yield loss, reduced fiber quality, and soil compaction, which collectively threaten long-term sustainability. To address these issues, this study proposes a lightweight, small-scale, vision-guided autonomous robotic cotton picker as an alternative. An autonomous system, built on Clearpath’s Husky platform and integrated with the CottonEye perception system, was developed and tested in the Gazebo simulation environment. A virtual cotton field was designed to facilitate autonomous navigation testing. The navigation system used Global Positioning System (GPS) and map-based guidance, assisted by an RGBdepth camera and a YOLOv8nseg instance segmentation model. The model achieved a mean Average Precision (mAP) of 85.2%, a recall of 88.9%, and a precision of 93.0%. The GPS-based approach reached a 100% completion rate (CR) within a threshold, while the map-based method achieved a 96.7% CR within a 0.25 m threshold. The developed Robot Operating System (ROS) packages enable robust simulation of autonomous cotton picking, offering a scalable baseline for future agricultural robotics. CottonSim code and datasets are publicly available on GitHub.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/blackberry.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="blackberry.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2025field" class="col-sm-8"> <div class="title">In-Field Multi-Ripeness Blackberry Detection for Soft Robotic Harvesting</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, Xin Zhang, Jonathan Harjono, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Yanbo Huang, Wenbo Liu, Amanda L McWhirt, Renee T Threlfall, Yue Chen' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Journal of the ASABE</em>. <em>More Information</em> can be <a href="https://github.com/Zhanglab-abe/Multi-Ripeness_Blackberry" rel="external nofollow noopener" target="_blank">found here</a> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.13031/ja.16300" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.13031/ja.16300" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.13031/ja.16300" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:0EnyYjriUFMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Blackberry harvesting is a crucial step for the fresh market production, requiring multiple passes of hand-picking because the berries do not ripen simultaneously, even on a plant, during the harvesting season. The blackberry harvesting process encounters several major hurdles in the U.S., including the shortage of agricultural labor and postharvest fruit quality since the blackberries are highly delicate and sensitive. Developing a computer vision-enabled robotic system for selectively picking blackberries can mitigate issues and secure the profitability of fresh blackberry growers. In-field blackberry detection and localization are extremely challenging attributable to several driving factors, such as the small size of the target berries, multiple levels of berry ripeness, and great variation of outdoor lighting conditions. This study aims to assess and compare the feasibility, accuracy, and efficiency of a series of state-of-the-art YOLO (You Only Look Once) models in detecting multi-ripeness blackberries in the farm conditions. A total of 1,086 images containing three different ripeness levels of blackberries were observed during the two-year harvesting season, including ripe berries (in black color), berries in the ripening stage (in pink color), and unripe berries (in green color). The computer vision pipeline developed in this study had the ability to detect and localize all berries at different ripeness levels, while detecting the ripe berries only was a particular focus. Overall, nine YOLO models (i.e., YOLOv5-x6, YOLOv6-l6, YOLOv7-base, YOLOv7-x, YOLOv7-e6e, YOLOv8-n, YOLOv8-x, YOLOv12-n, and YOLOv12-x) were trained and validated using randomly partitioned 760 (70%) and 108 images (10%), respectively. Among these models, YOLOv7-base outperformed the others in balancing mean Average Precision (mAP) and frames-per-second (FPS) on the test set, which comprised 218 images (20%). More specifically, YOLOv7-base achieved the mAP of 91.1%, F1-score of 84.9%, and inference speed of 12.6 ms per image with 1,024 x 1,024 pixels across all classes of ripeness. In addition, its mAP on the ripe berries was 92.4%, making YOLOv7-base a reliable tool for near real-time, in-field blackberry detection for soft robotic selective harvesting.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #FFF7CC; --venue-color-dark: #736100;"> <div>Agricultural Robotics</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IFAC.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IFAC.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2025perception" class="col-sm-8"> <div class="title">Perception-enabled manipulator control for a robotic cotton picker with dual-side harvesting capability</div> <div class="author"> <em>Thevathayarajh Thayananthan</em> and Xin Zhang </div> <div class="periodical"> <em>IFAC-PapersOnLine</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.ifacol.2025.11.809" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The United States is one of the leading global cotton producers, with most production concentrated in the southern states. Cotton harvesting predominantly relies on expensive and heavy mechanized harvesters, posing afordability challenges for small-scale farmers. Additionally, these machines require defoliant chemicals and contribute to soil compaction, raising environmental sustainability concerns. This study presents a robotic control system developed for a Universal Robot’s UR5e manipulator, designed to integrate with an autonomous cotton picker capable of dual-side harvesting. A dedicated control algorithm was implemented and tested using dual ZED 2i stereo cameras for cotton boll detection. Preliminary results confirmed the system’s accuracy and efficiency, demonstrating strong potential for robotic cotton harvesting applications.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/book_1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="book_1.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gunaratnam2024computer" class="col-sm-8"> <div class="title">Computer vision in livestock management and production</div> <div class="author"> Abhiram Gunaratnam, <em>Thevathayarajh Thayananthan</em>, Kartheeswaran Thangathurai, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Briyangari Abhiram' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Engineering Applications in Livestock Production</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/B978-0-323-98385-3.00002-5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/B978-0-323-98385-3.00002-5" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/B978-0-323-98385-3.00002-5" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:YsMSGLbcyi4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-10-4285F4?logo=googlescholar&amp;labelColor=beige" alt="10 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Intensification of livestock production systems is mainly driven by increasing population and subsequent increase in food demand. Major challenges of an intensified system are monitoring of a large number of animals, fast disease spread, frequent monitoring of health status, scarcity of labor, and optimizing the resources to maximize the profit. Different modern technologies are used to overcome these challenges; computer vision and image analysis techniques are the primary ones. The computer vision approach has gained prominence in contemporary livestock farming and has become an essential component of the farming system in many developed nations. This chapter provides insights into various applications of computer vision and image processing technologies (tracking of individual animals, health and disease monitoring, body size and weight measurement, and analysis of milk and meat quality), limitations, research gaps, and future prospects. This technology reduces labor requirements, eases management, is more precise in monitoring, and acts as a supporting tool for decision-making. However, the need for substantial initial capital, technical knowledge, regular monitoring, breed or strain-specific calibrations, and selection of correct features are identified as challenges of this technology. Lacks automation in computer vision technology, inadequate application of artificial intelligence in thermal imaging, identifying the optimum features for detection of lameness and lack of user-friendly applications are a few identified research gaps. Although computer vision has been implemented in livestock farming, it is still at the intermediate level and the identified challenges and research gaps need to be addressed through research and development for full-fledged automated livestock management.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/book_1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="book_1.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tarafdar2024engineering" class="col-sm-8"> <div class="title">Engineering Applications in Livestock Production</div> <div class="author"> Ayon Tarafdar, Ashok Pandey, Gyanendra Kumar Gaur, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Mukesh Singh, Hari Om Pandey' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/C2021-0-01048-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/C2021-0-01048-8" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/C2021-0-01048-8" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:WF5omc3nYNoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Engineering Applications in Livestock Production covers the recent advancements and technological developments in the field of livestock production engineering in great detail. The major advances covered in this book include the use of artificial intelligence, image processing, Internet of Things, novel animal product processing technologies, farm automation systems, sensor technology, bioengineering practices and even engineered housing systems among others.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IEEE.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IEEE.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vimalambikaipakan2024traffic" class="col-sm-8"> <div class="title">Traffic sign recognition and auditory alert system for sri lankan drivers using deep-learning</div> <div class="author"> Geerthana Vimalambikaipakan, Chinthaka Amarasinghe, Tharangani Rajapaksha, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Thevathayarajh Thayananthan, Janotheepan Mariyathas' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 2024 International Research Conference on Smart Computing and Systems Engineering (SCSE)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/SCSE61872.2024.10550615" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/SCSE61872.2024.10550615" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/SCSE61872.2024.10550615" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Traffic signs are essential for road safety, guiding and informing drivers about rules and conditions. Ignoring signs or traffic rules can lead to accidents, causing property damage, and even deaths. To ensure safety, drivers must remain aware of traffic signs, obey their instructions, and drive responsibly. Notably, Sri Lanka has unique traffic signs compared to other countries. This study aims to develop a deep learning-based traffic sign alert system with good accuracy and efficiency in traffic sign recognition and enhance driver awareness by integrating an auditory alert system. This alert system was implemented under two main sections: object detection system and auditory alert system. The traffic signs dataset images were labeled and pre-processed using the Roboflow software tool. The You Only Look Once version 5 (YOLOv5) model was used to train the system, which allowed it to understand complicated patterns and attributes associated with 15 different sign classes. To improve driver awareness, an auditory alert system was developed using the PyDub library to generate real-time alerts in the form of voice messages. These alerts are triggered whenever a traffic sign is detected, giving an additional layer of information to the driver. On the test dataset, the integrated and fine-tuned YOLOv5 model achieved the F1 score of 90.09% and mean average precision (mAP) of 87.55% resulting in good evaluation metrics values for detection. This YOLOv5 model-based traffic sign auditory alert system was highly effective and efficient in increasing driver awareness and also helped to reduce road accidents.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/catfish.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="catfish.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2023automating" class="col-sm-8"> <div class="title">Automating catfish cutting process using deep learning-based semantic segmentation</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, Xin Zhang, Wenbo Liu, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Tianqi Yao, Yanbo Huang, Nuwan K Wijewardane, Yuzhen Lu' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In Sensing for Agriculture and Food Quality and Safety XV</em>. <em>More Information</em> can be <a href="https://github.com/Zhanglab-abe/Catfish-Segment" rel="external nofollow noopener" target="_blank">found here</a> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/12.2663370" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1117/12.2663370" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1117/12.2663370" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-2-4285F4?logo=googlescholar&amp;labelColor=beige" alt="2 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Mississippi and Alabama are the top two states producing and processing catfish in the United States, with the annual production of $382 million in 2022. The catfish industry supplies protein-rich catfish products to the U.S. market and contributes considerably to the development of the local economy. However, the traditional catfish processing heavily relies on human labors leading to a high demand of workforce in the processing facilities. De-heading, gutting, portioning, filleting, skinning, and trimming are the main steps of the catfish processing, which normally require blade-based cutting device (e.g., metal blades) to handle. The blade-based manual catfish processing might lead to product contamination, considerable fish meat waste, and low yield of catfish fillet depending on the workers’ skill levels. Furthermore, operating the cutting devices may expose the human labors to undesired work accidents. Therefore, automated catfish cutting process appears to be an alternative and promising solution with minimal involvement of human labors. To further enable, assist, and automate the catfish cutting technique in near real-time, this study presents a novel computer vision-based sensing system for segmenting the catfish into different target parts using deep learning and semantic segmentation. In this study, 396 raw and augmented catfish images were used to train, validate, and test five state-of-the-art deep learning semantic segmentation models, including BEiTV1, SegFormer-B0, SegFormer-B5, ViT-Adapter and PSPNet. Five classes were pre-defined for the segmentation, which could effectively guide the cutting system to locate the target, including the head, body, fins, tail of the catfish, and the image background. Overall, BEiTV1 demonstrated the poorest performance with 77.3% of mIoU (mean intersection-over-union) and 86.7% of MPA (mean pixel accuracy) among all tested models using the test data set, while SegFormer-B5 outperformed all others with 89.2% of mIoU and 94.6% of MPA on the catfish images. The inference speed for SegFormer-B5 was 0.278 sec per image at the resolution of 640x640. The proposed deep learning-based sensing system is expected to be a reliable tool for automating the catfish cutting process.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/blackberry.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="blackberry.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023multi" class="col-sm-8"> <div class="title">Multi-ripeness level blackberry detection using YOLOv7 for soft robotic harvesting</div> <div class="author"> Xin Zhang, <em>Thevathayarajh Thayananthan</em>, Muhammad Usman, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Wenbo Liu, Yue Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Autonomous Air and Ground Sensing Systems for Agricultural Optimization and Phenotyping VIII</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/12.2663367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1117/12.2663367" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1117/12.2663367" target="_blank" rel="noopener" aria-label="PlumX metrics" style="display:inline-flex; align-items:center; margin-left:6px;"> <span class="badge bg-secondary">PlumX</span> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MpKhKEUAAAAJ&amp;citation_for_view=MpKhKEUAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Blackberry crop production is an essential sector of high-value specialty crops. Blackberries are delicate and easy to be damaged during harvest process. Besides, the blackberries in an orchard are not ripe at the same time so that multiple passes of harvesting are often needed. Therefore, the production is highly labor intensive and could be addressed using robotic solutions while maintaining the post-harvest berry quality for desired profitability. To further empower the developed tendon-driven soft robotic gripper specifically designed for berries, this study aims at investigating a state-of-the-art deep-learning YOLOv7 for accurately detecting the blackberries at multi-ripeness level in field conditions. In-field blackberry localization is a challenging task since blackberries are small objects and differ in color due to various levels of ripeness. Furthermore, the outdoor light condition varies depending on the time of day/location. Our study focused on detecting in-field blackberries at multi-ripeness levels using the state-of-the-art YOLOv7 model. In total, 642 RGB images were acquired targeting the plant canopies in several commercial orchards in Arkansas. The images were augmented to increase the diversity of data set using various methods. There are mainly three ripeness levels of blackberries that can present simultaneously in individual plants, including ripe (in black color), ripening (in red color), and unripe berries (in green color). The differentiation of ripeness levels can help the system to specifically harvest the ripe berries, and to keep track of the ripening/unripe berries in preparation for the next harvesting pass. The aggregation of total number of berries at all ripeness levels can also help estimate the crop-load for growers. The YOLOv7 model with seven configurations and six variants were trained and validated with 431 and 129 images, respectively. Overall, results of the test set (82 images) showed that YOLOv7-base was the best configuration with mean average precision (mAP) of 91.4% and F1-score of 0.86. YOLOv7-base also achieved 94% of mAP and 0.93 of True Positives (TPs) for ripe berries, 91% and 0.88 for ripening berries, and 88% and 0.86 for unripe berries under the Intersection-over-Union (IoU) of 0.5. The inference speed for YOLOv7-base was 21.5 ms on average per image with 1,024x1,024 resolution.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #7AA2FF; --venue-color-dark: #00369f;"> <div>Robotics</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IEEE.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IEEE.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yapa2023leveraging" class="col-sm-8"> <div class="title">Leveraging Virtual Reality for Robot Manipulator Education</div> <div class="author"> Aroshi Yapa, Kushan Ratnayake, Chanaka Prasad Premarathna, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Thevathayarajh Thayananthan' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2023 Moratuwa Engineering Research Conference (MERCon)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/MERCon60487.2023.10355455" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Equipping students with hands-on experience and knowledge of robot manipulators is crucial as robotics becomes more applicable in Industry 4.0. However, the practical implementation of physical manipulators in educational settings is challenging due to high capital costs, operational expertise requirements, safety considerations, and remote accessibility. This paper introduces an approach to address these obstacles by implementing a Virtual Reality (VR) platform for studying robot manipulators. The paper details the conceptual methodology, containing five domains: 3D Modelling, Application of Mechanics, VR Development, VR Integration, and Application Development. The methodology shows the creation of three VR applications focusing on distinct learning scenarios. The first application offers an overview of manipulators, while the second illustrates a specific operation (pick-and-place), providing insights into the combined function of links and joints. The third application allows users to interact with the manipulator, facilitating the execution of programming tasks. The paper concludes by outlining the benefits and the features of a VR manipulator over a physical robot manipulator. Then, it outlines the future enhancements, including developing a digital twin for the VR manipulator and a series of laboratory experiments. The study illustrates a crucial step towards broadening the accessibility of robotics education.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #FAD7CF; --venue-color-dark: #663127;"> <div>IoT</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2023.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2023-1" class="col-sm-8"> <div class="title">IoT-based patient monitoring system for Sri Lanka</div> <div class="author"> L.A.M.G.S.K. Liyadipita, A.R.P.C.C.J. Amarasingha, <em>Thevathayarajh Thayananthan</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'T.U.K.S. Bandara' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International Research Conference 2023</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2023-1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The Internet of Things (IoT) has profoundly transformed the healthcare industry, introducing continuous remote patient monitoring to improve patient care, treatment outcomes, and costeffectiveness. This research project presents a comprehensive IoT-based patient monitoring system designed to enable doctors and healthcare professionals to remotely examine their patients anytime and anywhere, eliminating the need for physical presence for routine checkups and saving valuable time for both medical staff and patients. The system’s core objectives include the development of a hardware driver to monitor patient health, the creation of a web application for real-time data collection, the establishment of communication channels for relaying patient information to doctors and caretakers, and the implementation of data analysis to provide regular updates on patient conditions. This IoTdriven approach facilitates the organization and accessibility of patient details and reports for all patient care stakeholders. Central to the system’s implementation is the NodeMCU, which seamlessly integrates various sensors with the IoT infrastructure. Low-power sensors are utilized to gather patient data, which is then displayed through open-source software, specifically Thingspeak. The collected data is stored securely on personal computers and the cloud, while an Android app enables doctors and healthcare professionals to conveniently access and review patient data in real time. This research project significantly contributes to enhancing patient care and healthcare delivery by enabling informed decision-making based on real-time patient data. The IoT-based patient monitoring system presented here is a scalable and convenient solution for healthcare professionals to continuously monitor and care for bedridden patients, regardless of location. The seamless integration of IoT technologies and medical devices empowers doctors and nurses to utilize mobile devices, facilitating their participation in a global network of healthcare providers. This IoT-based patient monitoring system exemplifies the potential of IoT in revolutionizing healthcare delivery, increasing healthcare efficiency, and offering continuous care to bedridden patients. By providing remote monitoring and access to real-time patient data, the system paves the way for a new era of healthcare, characterized by enhanced patient outcomes and proactive medical interventions. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2023.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2023-2" class="col-sm-8"> <div class="title">Deep learning-based traffic sign recognition and auditory alert system for Sri Lankan drivers</div> <div class="author"> V. Geerthana, <em>Thevathayarajh Thayananthan</em>, A.R.P.C.C.J. Amarasinghe, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'M. Janotheepan, R.M.T. Lakmali' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In International Research Conference 2023</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2023-2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Traffic signs play a crucial role in keeping the roads safe and providing efficient awareness to drivers. They play a vital role in helping drivers, navigate and understand the rules of the road. Road accidents occur when drivers fail to follow traffic signs or disregard traffic rules, resulting in injuries, property damage, and even deaths. It’s important to always pay attention to traffic signs, follow the provided instructions, and drive responsibly. Sri Lanka stands out with its distinct traffic signs, differing significantly from those seen in other countries. This study aims to develop a deep learning-based traffic sign alert system with good accuracy and efficiency in traffic sign recognition and enhance driver awareness through the integration of an auditory alert system. This alert system was implemented under two main divisions: object detection system and auditory alert system. The traffic signs dataset images were labeled and preprocessed using the Roboflow software tool. The system was trained using the YOLOv5 model, allowing it to learn complex patterns and features associated with 15 different sign classes. To further enhance driver awareness, an auditory alert system was developed using the PyDub library to generate real-time alerts in the form of voice messages. These alerts are triggered whenever a traffic sign is detected, providing an additional layer of information to the driver. The integrated and fine-tuned YOLOv5 model achieved the F1 score of 90.09% and mean average precision (mAP) of 87.55% on the test dataset resulting in good evaluation metrics values for detection. This YOLOv5 model-based traffic sign auditory alert system was highly effective and efficient in enhancing driver awareness. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2023.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2023-3" class="col-sm-8"> <div class="title">Development of automated image capturing for Zebrafish embryo toxicity model</div> <div class="author"> M.M.R. Ahamed, C. Amarasinghe, <em>Thevathayarajh Thayananthan</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'D.S.P.P.G. De Silva, K. De Zoysa, D.P.N. De Silva' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In International Research Conference 2023</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2023-3.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Zebrafish embryo is considered as one of the most suitable alternatives to animals in toxicity testings due to their special features like a transparent embryo, high fecundity (200-250 eggs), and the short period of embryonic development. The main problem with Zebrafish embryo toxicity model is the manual image inspection. The process is complex and unfeasible sometimes leading to misunderstanding sub-lethal endpoints. To help address this problem, this study aimed to develop a deep-learning model to analyze images. The deep-learning model was developed to detect seven embryonic development stages and ten morphological features of Zebrafish using the YOLOv5 algorithm. Different augmentation and preprocessing methods were used to improve the accuracy. The developed and fine-tuned model performed well with the mAP (Mean Average Precision) of over 85% in detecting most of the embryonic development stages. But it had mAP values of less than 80% in detecting the morphological features. The study results have shown that the proposed deep-learning model is a very promising step in detecting embryonic development stages and needs minor improvement in detecting morphological features. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2022-1" class="col-sm-8"> <div class="title">Automatic billing system for Sri Lankan pastry shop</div> <div class="author"> D.G.H. Jayawardhana, <em>Thevathayarajh Thayananthan</em>, R.M.T.C.B. Ekanayake, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'D.D.B. Senanayake' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International Research Conference 2022</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2022-1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With the advancement of information technology and artificial intelligence, employing science and technology to improve the food industry’s low efficiency is a very effective approach. Many cafeterias in Sri Lanka and other countries have long queues for food payments because of the high volume of customers at particular times of the day. Queues can occur when the demand for a service exceeds the facility’s ability to supply it. Most bakery goods and pastries in Sri Lanka are unique, and there is still no trained data set for identifying Sri Lankan pastry or bakery items. This paper solves this issue by including real-time image recognition techniques in the procedure. It is possible to eliminate the need for manual price computations by employing a camera to shoot a live picture at the checkout counter with an image recognition model, which produces the total invoice automatically. The recognition capacity of models determines the actual benefit of these systems under unconstrained conditions. A real-world dataset was gathered for testing the algorithms. The images were captured in a real bakery shop, with pastries arranged in various ways on a tray. Each tray can hold between one and seven pastries. A collection of ten different categories was gathered. TensorFlow SSD MobileNet V1 was used to train, validate, and test the image recognition model, including 2000 dataset images. The overall technique can be defined as detecting Sri Lankan pastries using Convolutional Neural Networks and developing a user interface in Python using Tkinter. According to the experimental data, the recognition accuracy of individual entrees was around 90%, and that of the full tray was approximately 95%. The advanced training may improve the model’s accuracy on a larger dataset, and using the approach during the checkout will become more practicable. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2022-2" class="col-sm-8"> <div class="title">Motorbike assistance tool using image processing technique</div> <div class="author"> D.M.G.P. Alwis, <em>Thevathayarajh Thayananthan</em>, R.M.T.C.B. Ekanayake, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'D.D.B. Senanayake' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International Research Conference 2022</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2022-2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Accidents involving motor vehicles account for a significant number of deaths and injuries that occur each year in Sri Lanka. The rider’s failure to be aware of vehicles following him and his inability to accurately estimate whether or not they will pass are two factors that frequently lead to collisions involving motorcycles. Even though there are several different technologies that can detect vehicles and lanes, the vast majority of them are not built for motorcyclists and even those that have additional drawbacks. This study proposed to create and develop a motorbike assistance tool that makes use of image processing techniques for road line recognition and behind vehicle detection in order to lower the average number of accidents that involve motorbikes each day. The novelty of this motorbike assistant tool is its ability of behind vehicle detection with the middle line detection. The Python programming language and the Open-CV library were utilized during the creation of this auxiliary tool. This program used the Counter Operation algorithm, which includes the open-cv library, to detect the behind lines. The open-cv package was also a part of this detection process. The TensorFlow object detection module was utilized largely for the purpose of recognizing vehicles from the back. A mobile application was created with Flutter to display those data. Using an ESP 32 camera, the hardware for video capture was developed. The ESP 32 camera and the mobile application were connected for final output. In addition to displaying the names of vehicles that were following the motorcycle, it also displayed the distance between the motorcycle and the center line of the road. According to the results, ninety percent of the attempts to detect the road lines were successful. Nevertheless, the identification of vehicles left behind was successful in a total of seventy percent of the cases. Some improvements such as solving the problem of detecting vehicles that pass the motorbike while coming from the front side should be done to this assistance tool. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #FAD7CF; --venue-color-dark: #663127;"> <div>IoT</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2022-3" class="col-sm-8"> <div class="title">Remote monitoring and controlling of an automobile system using NodeMCU</div> <div class="author"> D.M.K.C. Dissanayaka, A.C. Vidanapathirana, and <em>Thevathayarajh Thayananthan</em> </div> <div class="periodical"> <em>In International Research Conference 2022</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2022-3.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Developing of remote monitoring and control systems for vehicles has been a significant concern in the automobile industry. It may be quite useful if we can monitor and track the movements of the vehicles remotely. This will help in the case of vehicle thefts. Further, if the vehicle key is not physically available, it is better if we can have a method to unlock the car and start the journey. There are vehicle tracking systems in the market. Howevr, their customization, investment and system reliability are few of the concerns for the system users. This paper presents design and implementation of a remote monitoring and controlling system for vehicles. An embedded remote controlling (doors unlocking and locking, park lights, power windows, engine start etc.) and monitoring system were designed and installed in a real car. The remote controlling was achieved using mobile Wi-Fi and Android applications of smart phones. Android Studio based mobile application sends control commands to the NodeMCU device through a Wi-Fi network. Then the microcontroller mounted in the vehicle responded to these incoming commands. A GPS based position tracker system was integrated using Internet of Things (IoT) and Wi-Fi enabled module and NodeMCU. The monitoring system also provided the vehicle background information like temperature and humidity. The mobile application was developed using the firebase database which acts as a medium for data transfer and visualization. This technology will help the user to remotely control and track their vehicles using a mobile application. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2022-4" class="col-sm-8"> <div class="title">Development of an IoT based automated colony counter</div> <div class="author"> W.P.A.C. Sampath, <em>Thevathayarajh Thayananthan</em>, M. Prematilake, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'R.M.T.C.B. Ekanayake' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International Research Conference 2022</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2022-4.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Microorganisms are small-sized organisms that cannot be seen without a microscope. They grow on solid media as colonies. A colony is defined as the visible mass of microorganisms originating from a single mother cell. Recent studies have shown that temperature, humidity, time, and many other factors affect the growth of microorganisms. Scientists use incubators which are insulated enclosures to regulate humidity, temperature, and other environmental conditions at optimal levels for the growth and reproduction of microorganisms. When considering microbiological research, most depend on an accurate count of bacterial and fungal colonies. Colony forming unit (CFU) is indispensable in estimating microbial content, measuring cytotoxicity, and functions of specific genes. Therefore, researchers have to enumerate these colonies manually. Traditional manual methods are time-consuming, tedious, and error prone. A colony counter is an instrument used to count microbial colonies on a petri dish. Some automated colony counters based on image processing techniques are already available in the market. Not only that, some researchers have developed algorithms and methods to enumerate the microbial colonies count. However, in all these colony counters, researchers have to move the petri dish to the colony counter from the incubator to count the colonies. When researchers want to enumerate colony counting several times, it is subjective, and the changing environmental conditions have highly affected the growth of microorganisms and the final result of research. Therefore, this paper introduced an IoT-based automated colony counter that can place inside the incubator as well as enumerate and upload colony counting data to a web server (Google Drive API and Google Sheet API) in real-time using an IoT-based (WIFI) ESP32 camera module and video processing (OPEN-CV Python), with interfaces (PyQt5) using the laptop-computer to evade the problems mentioned above. The 3-D model of the counter was designed using CAD software and printed using the 3-D printer with PLA material. This device is the world’s first realtime updating IoT-based automated microbial colony counter that can be placed inside the incubator with a dedicated application for distanced monitoring. The accuracy of the novel colony counter was above 95% in identifying and counting colonies and it is more accurate compared with the manual colony counters.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2022-5" class="col-sm-8"> <div class="title">Recognition of Sinhala machine-printed text for postal address interpretation and postal automation</div> <div class="author"> A.M.P.R.B. Arawa, <em>Thevathayarajh Thayananthan</em>, Y. Mehendran, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'D.B.B. Senanayake' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International Research Conference 2022</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2022-5.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>While other Sri Lankan sectors are automating, the Sri Lankan postal system still uses manual intervention for mail sorting and processing. It takes more time to sort the mail according to the postal codes in the central nail exchange, even with the staff having a lot of experience and with the high number of employees while working overtime. The Sinhala language is used by the majority of Sri Lankans in their daily lives. On the other hand, less research has been done on Sinhala letter identification. Several systems have been established for this purpose in other languages including English. However, these types of systems are not available much in Sinhala due to the complexity of the language. Still, the findings have not been highlighted except in the above-mentioned research. Optical Character Recognition (OCR) and image processing technologies were used in the proposed system to recognize Sinhala printed addresses. The Google Tesseract was utilized to produce better optimal results faster and more accurately. Training, testing, and validation were done for the images taken from the printed postal envelopes. The model was trained and tested using the image data obtained under various criteria. Out of 15 Sinhala fonts, this system had an accuracy of 86.67%. A particular type of format was used to write the given addresses. This system can be expanded to include other formats in the future to automate the postal address classification system completely. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IRSUWU2022-6" class="col-sm-8"> <div class="title">Development of wind turbine system for electric and hybrid cart</div> <div class="author"> S.A.V. Gayashan, A.C. Vidanapathirana, R.M.T.C.B. Ekanayake, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Thevathayarajh Thayananthan' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International Research Conference 2022</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRSUWU2022-6.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Electric and hybrid vehicles are emerging as solutions to fossil fuel shortages. However, to travel a long distance, battery power may be insufficient, and electric car recharge durations may be lengthy. Previous research on wind turbine technologies had a direct impact on the appearance of the car. This research paper studied the development of a vehicle-mounted horizontal axis wind turbine for use in electric and hybrid vehicles. In the research horizontal axis, Archimedes wind turbine was used since it can effectively handle the urban wind conditions and it further has the property of drawing air stream into the turbine. Two air turbines will be mounted on the front bumper. These air turbines are expected to use wind energy to charge the battery of the electric or hybrid vehicle and increase the driving range. The objectives were to design the wind turbines, a sedan-type car that was modified and that could be simulated and tested in the field. Five-blade and three air foil wind turbines were developed using Qblade software. The Archimedes wind turbine models were then developed using Solidworks software. The drag force increased as the Archimedes wind turbine angle increased. The rotation speed decreased as the angle decreased. As a result, two separate average values were used. The size and number of wind turbines were determined by the type of the automobile and the size of the front bumper. The three-blade Archimedes wind turbine gave a better power coefficient and aerodynamics performance. The same turbine was tested in the field and showed positive results. As a limitation of this study, it was found that the wind turbine-mounted vehicle was not performing well under flat and ascending road conditions due to drag force on the vehicle. Therefore, vehicles with wind turbines can operate effectively when the car is descending. Further, the turbine system can also operate effectively while the vehicle is stopped since it can operate at low wind speeds and can operate while breaking the car. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #FAD7CF; --venue-color-dark: #663127;"> <div>IoT</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2021.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2021.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="danijal2021development" class="col-sm-8"> <div class="title">Development of Tracking over Speed System Using IoT Technology for Vehicles</div> <div class="author"> TJ Danijal and T Thevathayarajh </div> <div class="periodical"> <em></em> 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRCUWU2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Now, IoT technology is used for many applications. But they are certainly used for agriculture purpose in Sri Lanka. In the modern era, many accidents occur due to over speed. But accidents are controlled with the help of the police force and speed bumps in the past. IoT technology was used in this project to monitor the high speed. First, the velocity of the motorcycle was measured with the help of a Tachometer. When approaching the predefined maximum speed, the warning message was sent to the driver. When crossing the predefined maximum speed, all details including the reading of the Tachometer and location were sent to the mobile number of the motorcyclist and to the web application that was created for the police force at the same time using GPS/GPRS/GSM module. Finally, the output of the complete project shows 90% accuracy and the system showed the expected output with the available components. Keywords: IoT technology; over speed; Tachometer; Gps module; GPRS module; GSM module</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2020.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="activeddbs" class="col-sm-8"> <div class="title">Active and Passive Safety System for Differently Abled People and Adults</div> <div class="author"> DDB Senanayake, KWSN Kumari, and <em>Thevathayarajh Thayananthan</em> </div> <div class="periodical"> <em>In International Research Conference 2020</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRCUWU2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>15% of the world population encompasses the differently-abled community of a diversified range. It is a vivid fact that enough attention is not being paid towards the differently-abled ones who are residing within the residence, such where the guardian is not available. Hence research was conducted to produce a developed asset that supports in detecting and generating a signal during where the utmost care and attention are required. The developed asset is carried out as an oriented scenario of assistive technology being supported by video and image processing. The potential study in this regard is almost a success and improvements can be done by adding some advanced features such as facial expression detection system and emergency alert on the health care provider. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #FAD7CF; --venue-color-dark: #663127;"> <div>IoT</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2020.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="activeddbt" class="col-sm-8"> <div class="title">Active and Passive Safety System for Differently Abled People and Adults</div> <div class="author"> DDB Senanayake, KWSN Kumari, and <em>Thevathayarajh Thayananthan</em> </div> <div class="periodical"> <em>In International Research Conference 2020</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRCUWU2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>15% of the world population encompasses the differently-abled community of a diversified range. It is a vivid fact that enough attention is not being paid towards the differently-abled ones who are residing within the residence, such where the guardian is not available. Hence research was conducted to produce a developed asset that supports in detecting and generating a signal during where the utmost care and attention are required. The developed asset is carried out as an oriented scenario of assistive technology being supported by video and image processing. The potential study in this regard is almost a success and improvements can be done by adding some advanced features such as facial expression detection system and emergency alert on the health care provider. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge"> Mini-Project </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2019.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2019.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thevathayarajh2019digital" class="col-sm-8"> <div class="title">A Digital Device to Measure Distance on Multiple Surfaces</div> <div class="author"> T Thevathayarajh, ARPCCJ Amarasinghe, and RMTCB Ekanayake </div> <div class="periodical"> <em></em> 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRCUWU2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Measurements are one of the daily procedures to separate the needed length in apparel industries. Measuring tape is the only available method for this process now a days. Even shoulder measurements in tailor shop is taken by measuring tape too. This might consumes time and causes error while taking measurements quickly. So there is a need for the independent measuring system to solve this deficiency. A pen like structure of diameter 3.5cm cylindrical wooden bar was cut and further diameter of 2cm was drilled in that where the components were set. A Keyes KY-040, rotary encoder was used to count the rotation was attached to the end of the cylinder. Smooth surface and rough surface shaped disc were made using 3D printers and those models were designed using SolidWorks software. These discs can be attached to the encoder shaft according to the surface where the device is used. Arduino Nano was used as microcontroller for performing algorithmic work. Liquid crystal display was used to display the output measurement. This measurement can be displayed in Centimeter, Foot and Inch scales at the press of a button. Error corrections were done according to the measurements obtained from the Measuring device. 96% percentage accuracy was obtained on the random measurements</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/Indian_conf.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Indian_conf.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2019digital" class="col-sm-8"> <div class="title">A digital device to monitor the colour changes during fermentation stage of black tea processing</div> <div class="author"> <em>Thevathayarajh Thayananthan</em>, D.D.C. Wanniarachchi, and K.W.S.N. Kumari </div> <div class="periodical"> <em>In International Conference on Recent Advances in Computer Science and Engineering (IC-RACE) 2019</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Indian_conf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The tea industry is one of the most economically important agricultural sectors worldwide, where tea quality is primarily influenced by the fermentation process. Accurate assessment of tea fermentation is essential for maintaining consistent quality and is commonly evaluated using Theaflavin (TF) and Thearubigin (TR) ratios, with high-quality tea typically exhibiting TF/TR ratios between 1 and 1/10. These quality indicators are derived from polyphenols formed during fermentation and are affected by environmental and chemical conditions within the fermentation bed. In practice, tea makers rely on visual inspection of leaf colour changes to estimate fermentation completion, a method that depends heavily on experience and may lead to inconsistencies. This study proposes a novel sensor-based approach to establish a quantitative relationship between leaf colour variation and fermentation conditions. A device equipped with sensors measuring temperature, humidity, moisture, and colour was developed and deployed on the fermentation bed. Data were collected at five-minute intervals from multiple fermentation batches, with colour variations monitored across the fermentation duration. Reference samples were simultaneously analysed using spectrophotometric methods to obtain TF and TR values, along with UV spectral data. The collected data were statistically analysed to predict the relationship between fermentation parameters and tea quality indicators. Results show that average temperature, temperature variation, and room conditions significantly influence fermentation time at a 0.05 significance level, with the proposed method achieving an R² value of 96.6%. The findings demonstrate that sensor-driven colour analysis provides a reliable and objective alternative for monitoring tea fermentation and predicting optimal fermentation completion.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-badge" style="--venue-color-light: #5CDB95; --venue-color-dark: #009f36;"> <div>Computer Vision</div> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/IRCUWU2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IRCUWU2018.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thayananthan2018development" class="col-sm-8"> <div class="title">Development of a monitoring device for fermentation stage of black tea manufacturing</div> <div class="author"> <em>Thevathayarajh Thayananthan</em> and D.D.C. Wanniarachchi </div> <div class="periodical"> <em>In 2nd International Research Symposium</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IRCUWU2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Tea is one of the popular industries around the world for its social influence. Taste,tea colour and odor are the ways to measure the quality on the final product, but Theaflavins/Thearubigins ratio is accepted as 1/10 for a high quality. In general factory officers estimate the optimum fermentation time visually. However, optimum fermentation time might depend on humidity, temperature, moisture content of leaves, which are processing on a particular day. Thus, there is a need for a system independent from human decision. An electronic device was developed, in order to determine the optimum fermentation time for the black tea. A set of sensors such as humidity, moisture and temperature were attached to the device to store the physical environment data of the fermentation bed. Colour changes during the fermentation were monitored using an iPhone 6s camera, which have 12 mega pixels. First set of tea particles from a batch of fermentation was selected for the research. This device was allowed to collect data until the factory officer asked to stop the fermentation according to his own decision. A sample of each monitored batch was collected after firing to measure percentage of Theaflavins. Tea infusion was monitored using Ultraviolet spectrophotometer. Finally, data were analyzed statistically. Theaflavins content decreases with the fermentation time which is the trend expected. The average temperature, temperature difference and average room temperature are statistically significant with fermentation time at 0.05 level of significance and percentage of variance is 96.6. Moisture content is constant because this is expected as we focused on dry season leaves. Finally, study confirms the importance of measuring physical parameters when monitoring fermentation stage to obtain quality tea. Some advanced test has to be done on the fermentation tea colour in future. </p> </div> </div> </div> </li></ol> </div> </article> </div> <div class="social text-center mt-5"> <div class="contact-icons"> <a href="https://imtheva.github.io/blog/" title="Blogger"><i class="fa-brands fa-blogger-b"></i></a> <a href="/cv/" title="CV" target="_blank"><i class="ai ai-cv"></i></a> <a href="mailto:%74%68%65%76%61%31%39%39%33@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/imtheva" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/imtheva" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://medium.com/@theva1993" title="Medium" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-medium"></i></a> <a href="https://orcid.org/0009-0007-2576-4348" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://www.researchgate.net/profile/Thevathayarajh-Thayananthan/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://scholar.google.com/citations?user=MpKhKEUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note">Preferred contact via email theva@uga.edu or theva1993@gmail.com </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Thevathayarajh Thayananthan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 26, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>