---
---
@article{thevathayarajh2019digital,
  title={A Digital Device to Measure Distance on Multiple Surfaces},
  author={Thevathayarajh, T and Amarasinghe, ARPCCJ and Ekanayake, RMTCB},
  year={2019},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Measurements are one of the daily procedures to separate the needed length in apparel industries. Measuring tape is the only available method for this process now a days. Even shoulder measurements in tailor shop is taken by measuring tape too. This might consumes time and causes error while taking measurements quickly. So there is a need for the independent measuring system to solve this deficiency. A pen like structure of diameter 3.5cm cylindrical wooden bar was cut and further diameter of 2cm was drilled in that where the components were set. A Keyes KY-040, rotary encoder was used to count the rotation was attached to the end of the cylinder. Smooth surface and rough surface shaped disc were made using 3D printers and those models were designed using SolidWorks software. These discs can be attached to the encoder shaft according to the surface where the device is used. Arduino Nano was used as microcontroller for performing algorithmic work. Liquid crystal display was used to display the output measurement. This measurement can be displayed in Centimeter, Foot and Inch scales at the press of a button. Error corrections were done according to the measurements obtained from the Measuring device. 96% percentage accuracy was obtained on the random measurements},
  preview={IRCUWU2019.png},
  pdf={IRCUWU2019.pdf},
  abbr         = {Mini-Project}
}

@article{danijal2021development,
  title={Development of Tracking over Speed System Using IoT Technology for Vehicles},
  author={Danijal, TJ and Thevathayarajh, T},
  year={2021},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Now, IoT technology is used for many applications. But they are certainly used for agriculture purpose in Sri Lanka. In the modern era, many accidents occur due to over speed. But accidents are controlled with the help of the police force and speed bumps in the past. IoT technology was used in this project to monitor the high speed. First, the velocity of the motorcycle was measured with the help of a Tachometer. When approaching the predefined maximum speed, the warning message was sent to the driver. When crossing the predefined maximum speed, all details including the reading of the Tachometer and location were sent to the mobile number of the motorcyclist and to the web application that was created for the police force at the same time using GPS/GPRS/GSM module. Finally, the output of the complete project shows 90% accuracy and the system showed the expected output with the available components. Keywords: IoT technology; over speed; Tachometer; Gps module; GPRS module; GSM module},
  preview={IRCUWU2021.png},
  pdf={IRCUWU2021.pdf},
  abbr         = {IoT}
}

@inproceedings{thayananthan2023automating,
  title={Automating catfish cutting process using deep learning-based semantic segmentation},
  author={Thayananthan, Thevathayarajh and Zhang, Xin and Liu, Wenbo and Yao, Tianqi and Huang, Yanbo and Wijewardane, Nuwan K and Lu, Yuzhen},
  booktitle={Sensing for Agriculture and Food Quality and Safety XV},
  volume={12545},
  pages={103--116},
  year={2023},
  organization={SPIE},
  doi          = {10.1117/12.2663370},
  google_scholar_id = {IjCSPb-OGe4C},
  dimensions   = {true},
  plumx        = {true},
  abstract     = {Mississippi and Alabama are the top two states producing and processing catfish in the United States, with the annual production of $382 million in 2022. The catfish industry supplies protein-rich catfish products to the U.S. market and contributes considerably to the development of the local economy. However, the traditional catfish processing heavily relies on human labors leading to a high demand of workforce in the processing facilities. De-heading, gutting, portioning, filleting, skinning, and trimming are the main steps of the catfish processing, which normally require blade-based cutting device (e.g., metal blades) to handle. The blade-based manual catfish processing might lead to product contamination, considerable fish meat waste, and low yield of catfish fillet depending on the workers’ skill levels. Furthermore, operating the cutting devices may expose the human labors to undesired work accidents. Therefore, automated catfish cutting process appears to be an alternative and promising solution with minimal involvement of human labors. To further enable, assist, and automate the catfish cutting technique in near real-time, this study presents a novel computer vision-based sensing system for segmenting the catfish into different target parts using deep learning and semantic segmentation. In this study, 396 raw and augmented catfish images were used to train, validate, and test five state-of-the-art deep learning semantic segmentation models, including BEiTV1, SegFormer-B0, SegFormer-B5, ViT-Adapter and PSPNet. Five classes were pre-defined for the segmentation, which could effectively guide the cutting system to locate the target, including the head, body, fins, tail of the catfish, and the image background. Overall, BEiTV1 demonstrated the poorest performance with 77.3% of mIoU (mean intersection-over-union) and 86.7% of MPA (mean pixel accuracy) among all tested models using the test data set, while SegFormer-B5 outperformed all others with 89.2% of mIoU and 94.6% of MPA on the catfish images. The inference speed for SegFormer-B5 was 0.278 sec per image at the resolution of 640x640. The proposed deep learning-based sensing system is expected to be a reliable tool for automating the catfish cutting process.},
  additional_info = {. *More Information* can be [found here](https://github.com/Zhanglab-abe/Catfish-Segment)},
  abbr         = {Computer Vision},
  preview={catfish.jpg}
}

@inproceedings{zhang2023multi,
  title={Multi-ripeness level blackberry detection using YOLOv7 for soft robotic harvesting},
  author={Zhang, Xin and Thayananthan, Thevathayarajh and Usman, Muhammad and Liu, Wenbo and Chen, Yue},
  booktitle={Autonomous Air and Ground Sensing Systems for Agricultural Optimization and Phenotyping VIII},
  volume={12539},
  pages={85--96},
  year={2023},
  organization={SPIE},
  doi          = {10.1117/12.2663367},
  google_scholar_id = {zYLM7Y9cAGgC},
  dimensions   = {true},
  plumx        = {true},
  abstract     = {Blackberry crop production is an essential sector of high-value specialty crops. Blackberries are delicate and easy to be damaged during harvest process. Besides, the blackberries in an orchard are not ripe at the same time so that multiple passes of harvesting are often needed. Therefore, the production is highly labor intensive and could be addressed using robotic solutions while maintaining the post-harvest berry quality for desired profitability. To further empower the developed tendon-driven soft robotic gripper specifically designed for berries, this study aims at investigating a state-of-the-art deep-learning YOLOv7 for accurately detecting the blackberries at multi-ripeness level in field conditions. In-field blackberry localization is a challenging task since blackberries are small objects and differ in color due to various levels of ripeness. Furthermore, the outdoor light condition varies depending on the time of day/location. Our study focused on detecting in-field blackberries at multi-ripeness levels using the state-of-the-art YOLOv7 model. In total, 642 RGB images were acquired targeting the plant canopies in several commercial orchards in Arkansas. The images were augmented to increase the diversity of data set using various methods. There are mainly three ripeness levels of blackberries that can present simultaneously in individual plants, including ripe (in black color), ripening (in red color), and unripe berries (in green color). The differentiation of ripeness levels can help the system to specifically harvest the ripe berries, and to keep track of the ripening/unripe berries in preparation for the next harvesting pass. The aggregation of total number of berries at all ripeness levels can also help estimate the crop-load for growers. The YOLOv7 model with seven configurations and six variants were trained and validated with 431 and 129 images, respectively. Overall, results of the test set (82 images) showed that YOLOv7-base was the best configuration with mean average precision (mAP) of 91.4% and F1-score of 0.86. YOLOv7-base also achieved 94% of mAP and 0.93 of True Positives (TPs) for ripe berries, 91% and 0.88 for ripening berries, and 88% and 0.86 for unripe berries under the Intersection-over-Union (IoU) of 0.5. The inference speed for YOLOv7-base was 21.5 ms on average per image with 1,024x1,024 resolution.},
  abbr         = {Computer Vision},
  preview={blackberry.jpg}
}

@inproceedings{yapa2023leveraging,
  title={Leveraging Virtual Reality for Robot Manipulator Education},
  author={Yapa, Aroshi and Ratnayake, Kushan and Premarathna, Chanaka Prasad and Thayananthan, Thevathayarajh},
  booktitle={2023 Moratuwa Engineering Research Conference (MERCon)},
  pages={544--549},
  year={2023},
  organization={IEEE},
  doi          = {10.1109/MERCon60487.2023.10355455},
  abstract     = {Equipping students with hands-on experience and knowledge of robot manipulators is crucial as robotics becomes more applicable in Industry 4.0. However, the practical implementation of physical manipulators in educational settings is challenging due to high capital costs, operational expertise requirements, safety considerations, and remote accessibility. This paper introduces an approach to address these obstacles by implementing a Virtual Reality (VR) platform for studying robot manipulators. The paper details the conceptual methodology, containing five domains: 3D Modelling, Application of Mechanics, VR Development, VR Integration, and Application Development. The methodology shows the creation of three VR applications focusing on distinct learning scenarios. The first application offers an overview of manipulators, while the second illustrates a specific operation (pick-and-place), providing insights into the combined function of links and joints. The third application allows users to interact with the manipulator, facilitating the execution of programming tasks. The paper concludes by outlining the benefits and the features of a VR manipulator over a physical robot manipulator. Then, it outlines the future enhancements, including developing a digital twin for the VR manipulator and a series of laboratory experiments. The study illustrates a crucial step towards broadening the accessibility of robotics education.},
  abbr         = {Robotics},
  preview={IEEE.jpg}
}

@incollection{gunaratnam2024computer,
  title={Computer vision in livestock management and production},
  author={Gunaratnam, Abhiram and Thayananthan, Thevathayarajh and Thangathurai, Kartheeswaran and Abhiram, Briyangari},
  booktitle={Engineering Applications in Livestock Production},
  pages={93--128},
  abstract={Intensification of livestock production systems is mainly driven by increasing population and subsequent increase in food demand. Major challenges of an intensified system are monitoring of a large number of animals, fast disease spread, frequent monitoring of health status, scarcity of labor, and optimizing the resources to maximize the profit. Different modern technologies are used to overcome these challenges; computer vision and image analysis techniques are the primary ones. The computer vision approach has gained prominence in contemporary livestock farming and has become an essential component of the farming system in many developed nations. This chapter provides insights into various applications of computer vision and image processing technologies (tracking of individual animals, health and disease monitoring, body size and weight measurement, and analysis of milk and meat quality), limitations, research gaps, and future prospects. This technology reduces labor requirements, eases management, is more precise in monitoring, and acts as a supporting tool for decision-making. However, the need for substantial initial capital, technical knowledge, regular monitoring, breed or strain-specific calibrations, and selection of correct features are identified as challenges of this technology. Lacks automation in computer vision technology, inadequate application of artificial intelligence in thermal imaging, identifying the optimum features for detection of lameness and lack of user-friendly applications are a few identified research gaps. Although computer vision has been implemented in livestock farming, it is still at the intermediate level and the identified challenges and research gaps need to be addressed through research and development for full-fledged automated livestock management.},
  year={2024},
  google_scholar_id={YsMSGLbcyi4C},
  plumx={true},
  doi={10.1016/B978-0-323-98385-3.00002-5},
  dimensions={true},
  publisher={Elsevier},
  preview={book_1.png},
  abbr={Computer Vision},
}



@misc{tarafdar2024engineering,
  title={Engineering Applications in Livestock Production},
  author={Tarafdar, Ayon and Pandey, Ashok and Gaur, Gyanendra Kumar and Singh, Mukesh and Pandey, Hari Om},
  year={2024},
  publisher={Elsevier},
  abstract={Engineering Applications in Livestock Production covers the recent advancements and technological developments in the field of livestock production engineering in great detail. The major advances covered in this book include the use of artificial intelligence, image processing, Internet of Things, novel animal product processing technologies, farm automation systems, sensor technology, bioengineering practices and even engineered housing systems among others.},
  google_scholar_id={WF5omc3nYNoC},
  preview={book_1.png},
  plumx={true},
  doi={10.1016/C2021-0-01048-8},
  dimensions={true},
  abbr={Computer Vision}
}

@inproceedings{vimalambikaipakan2024traffic,
  title={Traffic sign recognition and auditory alert system for sri lankan drivers using deep-learning},
  author={Vimalambikaipakan, Geerthana and Amarasinghe, Chinthaka and Rajapaksha, Tharangani and Thayananthan, Thevathayarajh and Mariyathas, Janotheepan},
  booktitle={2024 International Research Conference on Smart Computing and Systems Engineering (SCSE)},
  volume={7},
  abstract={Traffic signs are essential for road safety, guiding and informing drivers about rules and conditions. Ignoring signs or traffic rules can lead to accidents, causing property damage, and even deaths. To ensure safety, drivers must remain aware of traffic signs, obey their instructions, and drive responsibly. Notably, Sri Lanka has unique traffic signs compared to other countries. This study aims to develop a deep learning-based traffic sign alert system with good accuracy and efficiency in traffic sign recognition and enhance driver awareness by integrating an auditory alert system. This alert system was implemented under two main sections: object detection system and auditory alert system. The traffic signs dataset images were labeled and pre-processed using the Roboflow software tool. The You Only Look Once version 5 (YOLOv5) model was used to train the system, which allowed it to understand complicated patterns and attributes associated with 15 different sign classes. To improve driver awareness, an auditory alert system was developed using the PyDub library to generate real-time alerts in the form of voice messages. These alerts are triggered whenever a traffic sign is detected, giving an additional layer of information to the driver. On the test dataset, the integrated and fine-tuned YOLOv5 model achieved the F1 score of 90.09% and mean average precision (mAP) of 87.55% resulting in good evaluation metrics values for detection. This YOLOv5 model-based traffic sign auditory alert system was highly effective and efficient in increasing driver awareness and also helped to reduce road accidents.},
  pages={1--5},
  year={2024},
  organization={IEEE},
  preview={IEEE.jpg},
  google_scholar_id={_FxGoFyzp5QC},
  plumx={true},
  doi={10.1109/SCSE61872.2024.10550615},
  dimensions={true},
  abbr={Computer Vision},
}

@inproceedings{thayananthan2019digital,
  title={A digital device to monitor the colour changes during fermentation stage of black tea processing},
  author={Thayananthan, Thevathayarajh and Wanniarachchi, D.D.C. and Kumari, K.W.S.N.},
  booktitle={International Conference on Recent Advances in Computer Science and Engineering (IC-RACE) 2019},
  pages={75--79},
  year={2019},
  abstract={The tea industry is one of the most economically important agricultural sectors worldwide, where tea quality is primarily influenced by the fermentation process. Accurate assessment of tea fermentation is essential for maintaining consistent quality and is commonly evaluated using Theaflavin (TF) and Thearubigin (TR) ratios, with high-quality tea typically exhibiting TF/TR ratios between 1 and 1/10. These quality indicators are derived from polyphenols formed during fermentation and are affected by environmental and chemical conditions within the fermentation bed. In practice, tea makers rely on visual inspection of leaf colour changes to estimate fermentation completion, a method that depends heavily on experience and may lead to inconsistencies. This study proposes a novel sensor-based approach to establish a quantitative relationship between leaf colour variation and fermentation conditions. A device equipped with sensors measuring temperature, humidity, moisture, and colour was developed and deployed on the fermentation bed. Data were collected at five-minute intervals from multiple fermentation batches, with colour variations monitored across the fermentation duration. Reference samples were simultaneously analysed using spectrophotometric methods to obtain TF and TR values, along with UV spectral data. The collected data were statistically analysed to predict the relationship between fermentation parameters and tea quality indicators. Results show that average temperature, temperature variation, and room conditions significantly influence fermentation time at a 0.05 significance level, with the proposed method achieving an R² value of 96.6%. The findings demonstrate that sensor-driven colour analysis provides a reliable and objective alternative for monitoring tea fermentation and predicting optimal fermentation completion.},
  preview={Indian_conf.png},
  pdf={Indian_conf.pdf},
  abbr         = {Computer Vision}
}

@article{thayananthan2025cottonsima,
  title={CottonSim: Development of an autonomous visual-guided robotic cotton-picking system in the Gazebo},
  author={Thayananthan, Thevathayarajh and Zhang, Xin and Huang, Yanbo and Chen, Jingdao and Wijewardane, Nuwan K and Martins, Vitor S and Chesser, Gary D and Goodin, Christopher T},
  journal={arXiv preprint arXiv:2505.05317},
  year={2025},
  doi          = {10.48550/arXiv.2505.05317},
  additional_info = {. *More Information* can be [found here](https://github.com/imtheva/CottonSim)},
  abstract     = {Cotton is a major cash crop in the United States, with the country being a leading global producer and exporter. Nearly all U.S. cotton is grown in the Cotton Belt, spanning 17 states in the southern region. Harvesting remains a critical yet challenging stage, impacted by the use of costly, environmentally harmful defoliants and heavy, expensive cotton pickers. These factors contribute to yield loss, reduced fiber quality, and soil compaction, which collectively threaten long-term sustainability. To address these issues, this study proposes a lightweight, small-scale, vision-guided autonomous robotic cotton picker as an alternative. An autonomous system, built on Clearpath's Husky platform and integrated with the CottonEye perception system, was developed and tested in the Gazebo simulation environment. A virtual cotton field was designed to facilitate autonomous navigation testing. The navigation system used Global Positioning System (GPS) and map-based guidance, assisted by an RGBdepth camera and a YOLOv8nseg instance segmentation model. The model achieved a mean Average Precision (mAP) of 85.2%, a recall of 88.9%, and a precision of 93.0%. The GPS-based approach reached a 100% completion rate (CR) within a  threshold, while the map-based method achieved a 96.7% CR within a 0.25 m threshold. The developed Robot Operating System (ROS) packages enable robust simulation of autonomous cotton picking, offering a scalable baseline for future agricultural robotics. CottonSim code and datasets are publicly available on GitHub},
  abbr         = {Agricultural Robotics},
  preview={CottonSim.gif}
}

@article{thayananthan2025cottonsimb,
  title        = {CottonSim: A vision-guided autonomous robotic system for cotton harvesting in Gazebo simulation},
  author       = {Thayananthan, Thevathayarajh and Zhang, Xin and Huang, Yanbo and Chen, Jingdao and Wijewardane, Nuwan K and Martins, Vitor S and Chesser, Gary D and Goodin, Christopher T},
  journal      = {Computers and Electronics in Agriculture},
  volume       = {239},
  pages        = {110963},
  year         = {2025},
  publisher    = {Elsevier},
  selected     = {true},
  doi          = {10.1016/j.compag.2025.110963},
  google_scholar_id = {hqOjcs7Dif8C},
  dimensions   = {true},
  plumx        = {true},
  abstract     = {Cotton is a major cash crop in the United States, with the country being a leading global producer and exporter. Nearly all U.S. cotton is grown in the Cotton Belt, spanning 17 states in the southern region. Harvesting remains a critical yet challenging stage, impacted by the use of costly, environmentally harmful defoliants and heavy, expensive cotton pickers. These factors contribute to yield loss, reduced fiber quality, and soil compaction, which collectively threaten long-term sustainability. To address these issues, this study proposes a lightweight, small-scale, vision-guided autonomous robotic cotton picker as an alternative. An autonomous system, built on Clearpath's Husky platform and integrated with the CottonEye perception system, was developed and tested in the Gazebo simulation environment. A virtual cotton field was designed to facilitate autonomous navigation testing. The navigation system used Global Positioning System (GPS) and map-based guidance, assisted by an RGBdepth camera and a YOLOv8nseg instance segmentation model. The model achieved a mean Average Precision (mAP) of 85.2%, a recall of 88.9%, and a precision of 93.0%. The GPS-based approach reached a 100% completion rate (CR) within a  threshold, while the map-based method achieved a 96.7% CR within a 0.25 m threshold. The developed Robot Operating System (ROS) packages enable robust simulation of autonomous cotton picking, offering a scalable baseline for future agricultural robotics. CottonSim code and datasets are publicly available on GitHub.},
  additional_info = {. *More Information* can be [found here](https://github.com/imtheva/CottonSim)},
  abbr         = {Agricultural Robotics},
  preview={CottonSim.gif}

}


@article{thayananthan2025field,
  title={In-Field Multi-Ripeness Blackberry Detection for Soft Robotic Harvesting},
  author={Thayananthan, Thevathayarajh and Zhang, Xin and Harjono, Jonathan and Huang, Yanbo and Liu, Wenbo and McWhirt, Amanda L and Threlfall, Renee T and Chen, Yue},
  journal={Journal of the ASABE},
  volume={68},
  number={6},
  pages={1073--1089},
  year={2025},
  publisher={American Society of Agricultural and Biological Engineers},
  doi          = {10.13031/ja.16300},
  google_scholar_id = {0EnyYjriUFMC},
  dimensions   = {true},
  plumx        = {true},
  abstract     = {Blackberry harvesting is a crucial step for the fresh market production, requiring multiple passes of hand-picking because the berries do not ripen simultaneously, even on a plant, during the harvesting season. The blackberry harvesting process encounters several major hurdles in the U.S., including the shortage of agricultural labor and postharvest fruit quality since the blackberries are highly delicate and sensitive. Developing a computer vision-enabled robotic system for selectively picking blackberries can mitigate issues and secure the profitability of fresh blackberry growers. In-field blackberry detection and localization are extremely challenging attributable to several driving factors, such as the small size of the target berries, multiple levels of berry ripeness, and great variation of outdoor lighting conditions. This study aims to assess and compare the feasibility, accuracy, and efficiency of a series of state-of-the-art YOLO (You Only Look Once) models in detecting multi-ripeness blackberries in the farm conditions. A total of 1,086 images containing three different ripeness levels of blackberries were observed during the two-year harvesting season, including ripe berries (in black color), berries in the ripening stage (in pink color), and unripe berries (in green color). The computer vision pipeline developed in this study had the ability to detect and localize all berries at different ripeness levels, while detecting the ripe berries only was a particular focus. Overall, nine YOLO models (i.e., YOLOv5-x6, YOLOv6-l6, YOLOv7-base, YOLOv7-x, YOLOv7-e6e, YOLOv8-n, YOLOv8-x, YOLOv12-n, and YOLOv12-x) were trained and validated using randomly partitioned 760 (70%) and 108 images (10%), respectively. Among these models, YOLOv7-base outperformed the others in balancing mean Average Precision (mAP) and frames-per-second (FPS) on the test set, which comprised 218 images (20%). More specifically, YOLOv7-base achieved the mAP of 91.1%, F1-score of 84.9%, and inference speed of 12.6 ms per image with 1,024 x 1,024 pixels across all classes of ripeness. In addition, its mAP on the ripe berries was 92.4%, making YOLOv7-base a reliable tool for near real-time, in-field blackberry detection for soft robotic selective harvesting.},
  additional_info = {. *More Information* can be [found here](https://github.com/Zhanglab-abe/Multi-Ripeness_Blackberry)},
  abbr         = {Computer Vision},
  preview={blackberry.jpg},
  selected     = {true},
}

@article{thayananthan2025perception,
  title={Perception-enabled manipulator control for a robotic cotton picker with dual-side harvesting capability},
  author={Thayananthan, Thevathayarajh and Zhang, Xin},
  journal={IFAC-PapersOnLine},
  volume={59},
  number={23},
  pages={332--337},
  year={2025},
  publisher={Elsevier},
  doi          = {10.1016/j.ifacol.2025.11.809},
  abstract = {The United States is one of the leading global cotton producers, with most production concentrated in the southern states. Cotton harvesting predominantly relies on expensive and heavy mechanized harvesters, posing afordability challenges for small-scale farmers. Additionally, these machines require defoliant chemicals and contribute to soil compaction, raising environmental sustainability concerns. This study presents a robotic control system developed for a Universal Robot’s UR5e manipulator, designed to integrate with an autonomous cotton picker capable of dual-side harvesting. A dedicated control algorithm was implemented and tested using dual ZED 2i stereo cameras for cotton boll detection. Preliminary results confirmed the system’s accuracy and efficiency, demonstrating strong potential for robotic cotton harvesting applications.},
  abbr         = {Agricultural Robotics},
  preview={IFAC.png}
}


@inproceedings{thayananthan2018development,
  title={Development of a monitoring device for fermentation stage of black tea manufacturing},
  author={Thayananthan, Thevathayarajh and Wanniarachchi, D.D.C.},
  booktitle={2nd International Research Symposium},
  year={2018},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Tea is one of the popular industries around the world for its social influence. Taste,tea colour and odor are the ways to measure the quality on the final product, but Theaflavins/Thearubigins ratio is accepted as 1/10 for a high quality. In general factory officers estimate the optimum fermentation time visually. However, optimum fermentation time might depend on humidity, temperature, moisture content of leaves, which are processing on a particular day. Thus, there is a need for a system
independent from human decision. An electronic device was developed, in order to
determine the optimum fermentation time for the black tea. A set of sensors such as
humidity, moisture and temperature were attached to the device to store the physical
environment data of the fermentation bed. Colour changes during the fermentation
were monitored using an iPhone 6s camera, which have 12 mega pixels. First set of
tea particles from a batch of fermentation was selected for the research. This device
was allowed to collect data until the factory officer asked to stop the fermentation
according to his own decision. A sample of each monitored batch was collected after
firing to measure percentage of Theaflavins. Tea infusion was monitored using
Ultraviolet spectrophotometer. Finally, data were analyzed statistically. Theaflavins
content decreases with the fermentation time which is the trend expected. The average
temperature, temperature difference and average room temperature are statistically
significant with fermentation time at 0.05 level of significance and percentage of
variance is 96.6. Moisture content is constant because this is expected as we focused
on dry season leaves. Finally, study confirms the importance of measuring physical
parameters when monitoring fermentation stage to obtain quality tea. Some advanced
test has to be done on the fermentation tea colour in future.
},
  preview={IRCUWU2018.png},
  pdf={IRCUWU2018.pdf},
  abbr         = {Computer Vision}
}

@inproceedings{activeddbs,
  title={Active and Passive Safety System for Differently Abled People and Adults},
  author={DDB Senanayake and KWSN Kumari and Thayananthan, Thevathayarajh},
  booktitle={International Research Conference 2020},
  year={2020},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={15% of the world population encompasses the differently-abled community of a 
diversified range. It is a vivid fact that enough attention is not being paid towards the 
differently-abled ones who are residing within the residence, such where the guardian is 
not available. Hence research was conducted to produce a developed asset that supports 
in detecting and generating a signal during where the utmost care and attention are 
required. The developed asset is carried out as an oriented scenario of assistive 
technology being supported by video and image processing. The potential study in this 
regard is almost a success and improvements can be done by adding some advanced 
features such as facial expression detection system and emergency alert on the health 
care provider.
},
  preview={IRCUWU2020.png},
  pdf={IRCUWU2020.pdf},
  abbr         = {Computer Vision}
}

@inproceedings{activeddbs,
  title={Active and Passive Safety System for Differently Abled People and Adults},
  author={DDB Senanayake and KWSN Kumari and Thayananthan, Thevathayarajh},
  booktitle={International Research Conference 2020},
  year={2020},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={15% of the world population encompasses the differently-abled community of a 
diversified range. It is a vivid fact that enough attention is not being paid towards the 
differently-abled ones who are residing within the residence, such where the guardian is 
not available. Hence research was conducted to produce a developed asset that supports 
in detecting and generating a signal during where the utmost care and attention are 
required. The developed asset is carried out as an oriented scenario of assistive 
technology being supported by video and image processing. The potential study in this 
regard is almost a success and improvements can be done by adding some advanced 
features such as facial expression detection system and emergency alert on the health 
care provider.
},
  preview={IRCUWU2020.png},
  pdf={IRCUWU2020.pdf},
  abbr         = {IoT}
}

@inproceedings{IRSUWU2022-1,
  title={Automatic billing system for Sri Lankan pastry shop},
  author={D.G.H. Jayawardhana and Thayananthan, Thevathayarajh and R.M.T.C.B. Ekanayake and  D.D.B. Senanayake},
  booktitle={International Research Conference 2022},
  year={2022},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={With the advancement of information technology and artificial intelligence, employing science and 
technology to improve the food industry's low efficiency is a very effective approach. Many 
cafeterias in Sri Lanka and other countries have long queues for food payments because of the high 
volume of customers at particular times of the day. Queues can occur when the demand for a service 
exceeds the facility's ability to supply it. Most bakery goods and pastries in Sri Lanka are unique, and 
there is still no trained data set for identifying Sri Lankan pastry or bakery items. This paper solves 
this issue by including real-time image recognition techniques in the procedure. It is possible to 
eliminate the need for manual price computations by employing a camera to shoot a live picture at the 
checkout counter with an image recognition model, which produces the total invoice automatically. 
The recognition capacity of models determines the actual benefit of these systems under 
unconstrained conditions. A real-world dataset was gathered for testing the algorithms. The images
were captured in a real bakery shop, with pastries arranged in various ways on a tray. Each tray can 
hold between one and seven pastries. A collection of ten different categories was gathered. 
TensorFlow SSD MobileNet V1 was used to train, validate, and test the image recognition model, 
including 2000 dataset images. The overall technique can be defined as detecting Sri Lankan pastries 
using Convolutional Neural Networks and developing a user interface in Python using Tkinter. 
According to the experimental data, the recognition accuracy of individual entrees was around 90%, 
and that of the full tray was approximately 95%. The advanced training may improve the model's 
accuracy on a larger dataset, and using the approach during the checkout will become more 
practicable.
},
  preview={IRCUWU2022.png},
  pdf={IRSUWU2022-1.pdf},
  abbr         = {Computer Vision}
}

@inproceedings{IRSUWU2022-2,
  title={Motorbike assistance tool using image processing technique},
  author={D.M.G.P. Alwis and Thayananthan, Thevathayarajh and R.M.T.C.B. Ekanayake and  D.D.B. Senanayake},
  booktitle={International Research Conference 2022},
  year={2022},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Accidents involving motor vehicles account for a significant number of deaths and injuries that occur 
each year in Sri Lanka. The rider's failure to be aware of vehicles following him and his inability to 
accurately estimate whether or not they will pass are two factors that frequently lead to collisions 
involving motorcycles. Even though there are several different technologies that can detect vehicles 
and lanes, the vast majority of them are not built for motorcyclists and even those that have additional 
drawbacks. This study proposed to create and develop a motorbike assistance tool that makes use of 
image processing techniques for road line recognition and behind vehicle detection in order to lower 
the average number of accidents that involve motorbikes each day. The novelty of this motorbike 
assistant tool is its ability of behind vehicle detection with the middle line detection. The Python 
programming language and the Open-CV library were utilized during the creation of this auxiliary 
tool. This program used the Counter Operation algorithm, which includes the open-cv library, to 
detect the behind lines. The open-cv package was also a part of this detection process. The 
TensorFlow object detection module was utilized largely for the purpose of recognizing vehicles 
from the back. A mobile application was created with Flutter to display those data. Using an ESP 32 
camera, the hardware for video capture was developed. The ESP 32 camera and the mobile 
application were connected for final output. In addition to displaying the names of vehicles that were 
following the motorcycle, it also displayed the distance between the motorcycle and the center line of 
the road. According to the results, ninety percent of the attempts to detect the road lines were 
successful. Nevertheless, the identification of vehicles left behind was successful in a total of seventy 
percent of the cases. Some improvements such as solving the problem of detecting vehicles that pass 
the motorbike while coming from the front side should be done to this assistance tool.
},
  preview={IRCUWU2022.png},
  pdf={IRSUWU2022-2.pdf},
  abbr         = {Computer Vision}
}


@inproceedings{IRSUWU2022-3,
  title={Remote monitoring and controlling of an automobile system using NodeMCU},
  author={D.M.K.C. Dissanayaka and A.C. Vidanapathirana and Thayananthan, Thevathayarajh},
  booktitle={International Research Conference 2022},
  year={2022},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Developing of remote monitoring and control systems for vehicles has been a significant concern in 
the automobile industry. It may be quite useful if we can monitor and track the movements of the 
vehicles remotely. This will help in the case of vehicle thefts. Further, if the vehicle key is not 
physically available, it is better if we can have a method to unlock the car and start the journey. There 
are vehicle tracking systems in the market. Howevr, their customization, investment and system 
reliability are few of the concerns for the system users. This paper presents design and 
implementation of a remote monitoring and controlling system for vehicles. An embedded remote 
controlling (doors unlocking and locking, park lights, power windows, engine start etc.) and 
monitoring system were designed and installed in a real car. The remote controlling was achieved 
using mobile Wi-Fi and Android applications of smart phones. Android Studio based mobile 
application sends control commands to the NodeMCU device through a Wi-Fi network. Then the 
microcontroller mounted in the vehicle responded to these incoming commands. A GPS based 
position tracker system was integrated using Internet of Things (IoT) and Wi-Fi enabled module and 
NodeMCU. The monitoring system also provided the vehicle background information like 
temperature and humidity. The mobile application was developed using the firebase database which 
acts as a medium for data transfer and visualization. This technology will help the user to remotely 
control and track their vehicles using a mobile application.
},
  preview={IRCUWU2022.png},
  pdf={IRSUWU2022-3.pdf},
  abbr         = {IoT}
}

@inproceedings{IRSUWU2022-4,
  title={Development of an IoT based automated colony counter},
  author={W.P.A.C. Sampath and Thayananthan, Thevathayarajh and  M. Prematilake and  R.M.T.C.B. Ekanayake},
  booktitle={International Research Conference 2022},
  year={2022},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Microorganisms are small-sized organisms that cannot be seen without a microscope. They grow on 
solid media as colonies. A colony is defined as the visible mass of microorganisms originating from a 
single mother cell. Recent studies have shown that temperature, humidity, time, and many other 
factors affect the growth of microorganisms. Scientists use incubators which are insulated enclosures 
to regulate humidity, temperature, and other environmental conditions at optimal levels for the 
growth and reproduction of microorganisms. When considering microbiological research, most 
depend on an accurate count of bacterial and fungal colonies. Colony forming unit (CFU) is 
indispensable in estimating microbial content, measuring cytotoxicity, and functions of specific 
genes. Therefore, researchers have to enumerate these colonies manually. Traditional manual 
methods are time-consuming, tedious, and error prone. A colony counter is an instrument used to 
count microbial colonies on a petri dish. Some automated colony counters based on image processing 
techniques are already available in the market. Not only that, some researchers have developed 
algorithms and methods to enumerate the microbial colonies count. However, in all these colony 
counters, researchers have to move the petri dish to the colony counter from the incubator to count 
the colonies. When researchers want to enumerate colony counting several times, it is subjective, and 
the changing environmental conditions have highly affected the growth of microorganisms and the 
final result of research. Therefore, this paper introduced an IoT-based automated colony counter that 
can place inside the incubator as well as enumerate and upload colony counting data to a web server 
(Google Drive API and Google Sheet API) in real-time using an IoT-based (WIFI) ESP32 camera 
module and video processing (OPEN-CV Python), with interfaces (PyQt5) using the laptop-computer 
to evade the problems mentioned above. The 3-D model of the counter was designed using CAD 
software and printed using the 3-D printer with PLA material. This device is the world's first real￾time updating IoT-based automated microbial colony counter that can be placed inside the incubator
with a dedicated application for distanced monitoring. The accuracy of the novel colony counter was
above 95% in identifying and counting colonies and it is more accurate compared with the manual
colony counters.},
  preview={IRCUWU2022.png},
  pdf={IRSUWU2022-4.pdf},
  abbr         = {Computer Vision}
}


@inproceedings{IRSUWU2022-5,
  title={Recognition of Sinhala machine-printed text for postal address interpretation and postal automation},
  author={A.M.P.R.B. Arawa and Thayananthan, Thevathayarajh and  Y. Mehendran and  D.B.B. Senanayake},
  booktitle={International Research Conference 2022},
  year={2022},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={While other Sri Lankan sectors are automating, the Sri Lankan postal system still uses manual 
intervention for mail sorting and processing. It takes more time to sort the mail according to the 
postal codes in the central nail exchange, even with the staff having a lot of experience and with the 
high number of employees while working overtime. The Sinhala language is used by the majority of 
Sri Lankans in their daily lives. On the other hand, less research has been done on Sinhala letter 
identification. Several systems have been established for this purpose in other languages including 
English. However, these types of systems are not available much in Sinhala due to the complexity of 
the language. Still, the findings have not been highlighted except in the above-mentioned research. 
Optical Character Recognition (OCR) and image processing technologies were used in the proposed 
system to recognize Sinhala printed addresses. The Google Tesseract was utilized to produce better 
optimal results faster and more accurately. Training, testing, and validation were done for the images 
taken from the printed postal envelopes. The model was trained and tested using the image data 
obtained under various criteria. Out of 15 Sinhala fonts, this system had an accuracy of 86.67%. A 
particular type of format was used to write the given addresses. This system can be expanded to 
include other formats in the future to automate the postal address classification system completely.
},
  preview={IRCUWU2022.png},
  pdf={IRSUWU2022-5.pdf},
  abbr         = {Computer Vision}
}

@inproceedings{IRSUWU2022-6,
  title={Development of wind turbine system for electric and hybrid cart},
  author={S.A.V. Gayashan and A.C. Vidanapathirana and R.M.T.C.B. Ekanayake and  Thayananthan, Thevathayarajh},
  booktitle={International Research Conference 2022},
  year={2022},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Electric and hybrid vehicles are emerging as solutions to fossil fuel shortages. However, to travel a 
long distance, battery power may be insufficient, and electric car recharge durations may be lengthy. 
Previous research on wind turbine technologies had a direct impact on the appearance of the car. This 
research paper studied the development of a vehicle-mounted horizontal axis wind turbine for use in 
electric and hybrid vehicles. In the research horizontal axis, Archimedes wind turbine was used since 
it can effectively handle the urban wind conditions and it further has the property of drawing air 
stream into the turbine. Two air turbines will be mounted on the front bumper. These air turbines are 
expected to use wind energy to charge the battery of the electric or hybrid vehicle and increase the 
driving range. The objectives were to design the wind turbines, a sedan-type car that was modified 
and that could be simulated and tested in the field. Five-blade and three air foil wind turbines were 
developed using Qblade software. The Archimedes wind turbine models were then developed using 
Solidworks software. The drag force increased as the Archimedes wind turbine angle increased. The 
rotation speed decreased as the angle decreased. As a result, two separate average values were used. 
The size and number of wind turbines were determined by the type of the automobile and the size of 
the front bumper. The three-blade Archimedes wind turbine gave a better power coefficient and 
aerodynamics performance. The same turbine was tested in the field and showed positive results. As 
a limitation of this study, it was found that the wind turbine-mounted vehicle was not performing 
well under flat and ascending road conditions due to drag force on the vehicle. Therefore, vehicles 
with wind turbines can operate effectively when the car is descending. Further, the turbine system can 
also operate effectively while the vehicle is stopped since it can operate at low wind speeds and can 
operate while breaking the car.
},
  preview={IRCUWU2022.png},
  pdf={IRSUWU2022-6.pdf},
  abbr         = {Computer Vision}
}


@inproceedings{IRSUWU2023-1,
  title={IoT-based patient monitoring system for Sri Lanka},
  author={L.A.M.G.S.K. Liyadipita and A.R.P.C.C.J. Amarasingha and Thayananthan, Thevathayarajh and T.U.K.S. Bandara},
  booktitle={International Research Conference 2023},
  year={2023},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={The Internet of Things (IoT) has profoundly transformed the healthcare industry, introducing 
continuous remote patient monitoring to improve patient care, treatment outcomes, and cost￾effectiveness. This research project presents a comprehensive IoT-based patient monitoring system 
designed to enable doctors and healthcare professionals to remotely examine their patients anytime and 
anywhere, eliminating the need for physical presence for routine checkups and saving valuable time 
for both medical staff and patients. The system's core objectives include the development of a hardware 
driver to monitor patient health, the creation of a web application for real-time data collection, the 
establishment of communication channels for relaying patient information to doctors and caretakers, 
and the implementation of data analysis to provide regular updates on patient conditions. This IoT￾driven approach facilitates the organization and accessibility of patient details and reports for all patient 
care stakeholders. Central to the system's implementation is the NodeMCU, which seamlessly 
integrates various sensors with the IoT infrastructure. Low-power sensors are utilized to gather patient 
data, which is then displayed through open-source software, specifically Thingspeak. The collected 
data is stored securely on personal computers and the cloud, while an Android app enables doctors and 
healthcare professionals to conveniently access and review patient data in real time. This research 
project significantly contributes to enhancing patient care and healthcare delivery by enabling informed 
decision-making based on real-time patient data. The IoT-based patient monitoring system presented 
here is a scalable and convenient solution for healthcare professionals to continuously monitor and care 
for bedridden patients, regardless of location. The seamless integration of IoT technologies and medical 
devices empowers doctors and nurses to utilize mobile devices, facilitating their participation in a 
global network of healthcare providers. This IoT-based patient monitoring system exemplifies the 
potential of IoT in revolutionizing healthcare delivery, increasing healthcare efficiency, and offering 
continuous care to bedridden patients. By providing remote monitoring and access to real-time patient 
data, the system paves the way for a new era of healthcare, characterized by enhanced patient outcomes 
and proactive medical interventions.
},
  preview={IRCUWU2023.png},
  pdf={IRSUWU2023-1.pdf},
  abbr         = {IoT}
}

@inproceedings{IRSUWU2023-2,
  title={Deep learning-based traffic sign recognition and auditory alert system for Sri Lankan drivers},
  author={V. Geerthana and Thayananthan, Thevathayarajh and A.R.P.C.C.J. Amarasinghe and M. Janotheepan and R.M.T. Lakmali},
  booktitle={International Research Conference 2023},
  year={2023},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Traffic signs play a crucial role in keeping the roads safe and providing efficient awareness to drivers. 
They play a vital role in helping drivers, navigate and understand the rules of the road. Road accidents 
occur when drivers fail to follow traffic signs or disregard traffic rules, resulting in injuries, property 
damage, and even deaths. It's important to always pay attention to traffic signs, follow the provided 
instructions, and drive responsibly. Sri Lanka stands out with its distinct traffic signs, differing 
significantly from those seen in other countries. This study aims to develop a deep learning-based 
traffic sign alert system with good accuracy and efficiency in traffic sign recognition and enhance 
driver awareness through the integration of an auditory alert system. This alert system was 
implemented under two main divisions: object detection system and auditory alert system. The traffic 
signs dataset images were labeled and preprocessed using the Roboflow software tool. The system was 
trained using the YOLOv5 model, allowing it to learn complex patterns and features associated with 
15 different sign classes. To further enhance driver awareness, an auditory alert system was developed 
using the PyDub library to generate real-time alerts in the form of voice messages. These alerts are 
triggered whenever a traffic sign is detected, providing an additional layer of information to the driver. 
The integrated and fine-tuned YOLOv5 model achieved the F1 score of 90.09% and mean average 
precision (mAP) of 87.55% on the test dataset resulting in good evaluation metrics values for detection. 
This YOLOv5 model-based traffic sign auditory alert system was highly effective and efficient in 
enhancing driver awareness.
},
  preview={IRCUWU2023.png},
  pdf={IRSUWU2023-2.pdf},
  abbr         = {Computer Vision}
}

@inproceedings{IRSUWU2023-3,
  title={Development of automated image capturing for Zebrafish embryo toxicity model},
  author={M.M.R. Ahamed and  C. Amarasinghe and Thayananthan, Thevathayarajh and D.S.P.P.G. De Silva and K. De Zoysa and D.P.N. De Silva},
  booktitle={International Research Conference 2023},
  year={2023},
  publisher={Uva Wellassa University of Sri Lanka},
  abstract={Zebrafish embryo is considered as one of the most suitable alternatives to animals in toxicity testings 
due to their special features like a transparent embryo, high fecundity (200-250 eggs), and the short 
period of embryonic development. The main problem with Zebrafish embryo toxicity model is the 
manual image inspection. The process is complex and unfeasible sometimes leading to 
misunderstanding sub-lethal endpoints. To help address this problem, this study aimed to develop a 
deep-learning model to analyze images. The deep-learning model was developed to detect seven 
embryonic development stages and ten morphological features of Zebrafish using the YOLOv5 
algorithm. Different augmentation and preprocessing methods were used to improve the accuracy. The 
developed and fine-tuned model performed well with the mAP (Mean Average Precision) of over 85% 
in detecting most of the embryonic development stages. But it had mAP values of less than 80% in 
detecting the morphological features. The study results have shown that the proposed deep-learning 
model is a very promising step in detecting embryonic development stages and needs minor 
improvement in detecting morphological features.
},
  preview={IRCUWU2023.png},
  pdf={IRSUWU2023-3.pdf},
  abbr         = {Computer Vision}
}
